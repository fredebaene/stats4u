# Relationships and Regression

```{r echo=FALSE}
set.seed(678)
```

```{r echo=FALSE}
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(tibble))
```

```{r echo=FALSE}
data(Orange)
```

## Relationships

Two random variables are related if changes in one variable are consistently
associated with changes in another variable. Or, in other words, when the two
random variables tend to change together. We say that the two variables have a
relationship (or relation).

When describing relationships between variables, we must take into account (1)
the pattern, (2) the direction, and (3) the strength of the relationship. The
pattern of the relationship focuses on the shape of the regression curve of the
relationship. This can be linear, quadratic, exponential, etc. The relationship
can be positive or negative (direction). And the relationship can be strong or
weak. We will come back on the topic of strength of a relationship when
discussing the Pearson correlation coefficient.

As an example, let us have a look at the **Orange** dataset. This dataset
contains 35 observations on two random variables: age in days and circumference
in millimeters. In the following figure, we see that an increase in age tends to
be associated with an increase in circumference. Or, in other words, relatively
low values for age tend to occur with relatively low values for circumference,
and relatively high values for age tend to occur with relatively high values for
circumference. This indicates a positive relationship. When drawing a smoother
through the data, it seems the relationship can be described with an
approximately straight line, making it a linear relationship.

```{r}
ggplot(Orange, aes(x = age, y = circumference)) +
  geom_point() +
  geom_smooth(method = "loess", formula = "y ~ x", se = FALSE) +
  labs(x = "Age (days)", y = "Circumference (mm)") +
  theme_classic()
```

In the following sections, we will focus on statistical measures that can be
used to describe linear relationships.

## Pearson Correlation Coefficient

The Pearson correlation coefficient is a statistical measure that quantifies the
direction and strength of a linear relationship between two variables. The sign
of the Pearson correlation coefficient indicates the direction and the absolute
value, ranging from $0$ to $1$, indicates the strength of the relationship.

### Calculating Coefficient

The Pearson correlation coefficient is calculated as follows:

$$
\rho = \frac{Cov(X, Y)}{\sigma_{X} \sigma_{Y}} = \frac{\sigma_{XY}}{\sigma_{X} \sigma_{Y}}
$$

Note that $\rho$ denotes the population Pearson correlation coefficient, while
$r$ denotes the sample correlation coefficient. The latter is calculated as
follows:

$$
r = \frac{s_{XY}}{s_{X} s_{Y}}
$$

Dividing the covariance between $X$ and $Y$ by the product of the standard
deviations of $X$ and $Y$ ensures that the correlation coefficient is not
dependent on the scale of the variables $X$ and $Y$ and that $\rho \in [-1,
+1]$. If $\rho = +1$, then all of the data falls on a single straight line with
a positive slope. Some examples:

```{r}
# Create a sample dataset
dat <- tibble(X = runif(n = 20, min = 10, max = 80))
dat <- add_column(dat, Y1 = 0.30*dat$X)
dat <- add_column(dat, Y2 = 3.00*dat$X)
dat <- add_column(dat, Y3 = 30.0*dat$X)
```

```{r}
# Visualize the relationships
ggplot(dat, aes(x = X)) +
  geom_point(aes(y = Y1), color = "darkred") +
  geom_smooth(method = "lm", aes(y = Y1), formula = "y ~ x", color = "darkred", se = F) +
  geom_point(aes(y = Y2), color = "darkgreen") +
  geom_smooth(method = "lm", aes(y = Y2), formula = "y ~ x", color = "darkgreen", se = F) +
  geom_point(aes(y = Y3), color = "darkblue") +
  geom_smooth(method = "lm", aes(y = Y3), formula = "y ~ x", color = "darkblue", se = F) +
  labs(x = "X", y = "Y") +
  theme_classic()
```

Note that these examples illustrate a functional relationship. The relationship
between $X$ and each of the $Y$ variables ($Y1$, $Y2$, $Y3$) can be described
by an equation and all the observations fall on the line of the relationship. If
we compute the Pearson correlation coefficients for each of these three pairs
of variables, we see that it is each time equal to $r = +1$.

```{r}
cor(x = dat$X, y = dat$Y1, method = "pearson")
cor(x = dat$X, y = dat$Y2, method = "pearson")
cor(x = dat$X, y = dat$Y3, method = "pearson")
```

If we now use a negative slope, we will see that the Pearson correlation
coefficient for each of these functional relationships equals $r = -1$.

```{r}
# Create a sample dataset
dat <- tibble(X = runif(n = 20, min = 10, max = 80))
dat <- add_column(dat, Y1 = -0.30*dat$X)
dat <- add_column(dat, Y2 = -3.00*dat$X)
dat <- add_column(dat, Y3 = -30.0*dat$X)
```

```{r}
# Visualize the relationships
ggplot(dat, aes(x = X)) +
  geom_point(aes(y = Y1), color = "darkred") +
  geom_smooth(method = "lm", aes(y = Y1), formula = "y ~ x", color = "darkred", se = F) +
  geom_point(aes(y = Y2), color = "darkgreen") +
  geom_smooth(method = "lm", aes(y = Y2), formula = "y ~ x", color = "darkgreen", se = F) +
  geom_point(aes(y = Y3), color = "darkblue") +
  geom_smooth(method = "lm", aes(y = Y3), formula = "y ~ x", color = "darkblue", se = F) +
  labs(x = "X", y = "Y") +
  theme_classic()
```

```{r}
cor(x = dat$X, y = dat$Y1, method = "pearson")
cor(x = dat$X, y = dat$Y2, method = "pearson")
cor(x = dat$X, y = dat$Y3, method = "pearson")
```

We see that the absolute value of the Pearson correlation coefficient equals $1$
for the six examples. This indicates that these are very strong linear
relationships. But what does it mean for a relationship between two variables to
be strong?

The strength of a relationship between two variables is an indication of how
much information a given value for one variables gives us about the value of the
other variable. If we see that for a given value of $X$ the range of values for
$Y$ is relatively large, and this for every value of $X$, then we cannot say
that $X$ carries much information about $Y$. If, however, the range of values
for $Y$ is relatively small for every value of $X$, then we can state that $X$
carries a lot of information about the value of $Y$ and the relationship is
quite strong.

We will showcase with a few examples.

```{r}
# Create a sample dataset
dat <- tibble(X = runif(100, min = 20, max = 40))
dat <- add_column(dat, Y1 = -5 + 5*dat$X + rnorm(100, sd = 1))
dat <- add_column(dat, Y2 = -5 + 5*dat$X + rnorm(100, sd = 10))
dat <- add_column(dat, Y3 = -5 + 5*dat$X + rnorm(100, sd = 25))
```

For the following three positive linear relationships, we see that the amount of
scatter around the regression line is different for the three examples. The
amount of scatter around the regression line is an indication of how much
information the variable $X$ carries about the variable $Y$.

If we look at any given value for $X$, we see that the range of observed values
for $Y1$ is relatively small. For $Y2$, the range of observed $Y2$ values is
larger. And for $Y3$, the range of observed $Y3$ values is relatively large.
Therefore, we can state that the relationship between $X$ and $Y1$ is strong,
between $X$ and $Y2$ is moderate, and between $X$ and $Y3$ is weak.

```{r}
# Visualize the relationships
ggplot(dat, aes(x = X)) +
  geom_point(aes(y = Y1), color = "darkred") +
  geom_abline(intercept = -5, slope = 5, color = "darkred") +
  labs(x = "X", y = "Y1") +
  scale_y_continuous(limits = c(60, 250)) +
  theme_classic()
```

```{r}
# Visualize the relationships
ggplot(dat, aes(x = X)) +
  geom_point(aes(y = Y2), color = "darkgreen") +
  geom_abline(intercept = -5, slope = 5, color = "darkgreen") +
  labs(x = "X", y = "Y2") +
  scale_y_continuous(limits = c(60, 250)) +
  theme_classic()
```

```{r}
# Visualize the relationships
ggplot(dat, aes(x = X)) +
  geom_point(aes(y = Y3), color = "darkblue") +
  geom_abline(intercept = -5, slope = 5, color = "darkblue") +
  labs(x = "X", y = "Y3") +
  scale_y_continuous(limits = c(60, 250)) +
  theme_classic()
```

The absolute values of the Pearson correlation coefficient are indicative of
the strength of the linear relationship. Therefore, we expect the largest
Pearson correlation coefficient for $X$ and $Y1$, a smaller one for $X$ and
$Y2$, and the smallest one for $X$ and $Y3$.

```{r}
cor(x = dat$X, y = dat$Y1, method = "pearson")
cor(x = dat$X, y = dat$Y2, method = "pearson")
cor(x = dat$X, y = dat$Y3, method = "pearson")
```

### Hypothesis Testing

All of the above examples on the Pearson correlation coefficient made use of
samples of data to calculate the correlation. When analyzing data, we must be
be aware that often we are analyzing a sample of data which is drawn from a
larger population. For this population, there is a true Pearson correlation
coefficient $\rho$, which is an unknown constant. To estimate $\rho$, we draw
a random sample from the population and use an estimator to produce an estimate
$\hat{\rho}$. We have already seen the estimator in this chapter. It is:

$$
r = \frac{s_{XY}}{s_{X} s_{Y}}
$$

We must remember that the Pearson correlation coefficient $r$ is an estimator
for the true Pearson correlation coefficient $\rho$ in the population. The
latter is an unknown constant. We want to estimate this unknown constant and
therefore we sample data from the population and analyze this data to produce an
estimate of the true value of $\rho$.

Using this, we can perform a $t$-test to test for the existence of a linear
relationship between two variables. The null hypothesis states that the
population correlation coefficient $\rho = 0$ and that there is no linear
relationship:

$$
H_0 : \rho = 0
$$

The alternative hypothesis states:

$$
H_a : \rho \neq 0
$$

The test statistic $T$ for this test is:

$$
T = \frac{r \cdot \sqrt{n - 2}}{\sqrt{1 - R^2}}
$$

The test statistic $T$ follows a $t$-distribution with $n - 2$ degrees of
freedom.

```{r}
cor.test(x = dat$X, y = dat$Y1, method = "pearson")
cor.test(x = dat$X, y = dat$Y2, method = "pearson")
cor.test(x = dat$X, y = dat$Y3, method = "pearson")
```

In these three examples, we see that the $p$-values are all statistically
significant at $\alpha = 0.05$. If we look at the test statistic $T$, we see
that both sample size $n$ and the estimate $r$ affect the test statistic. We
see that larger sample sizes and larger estimates of $\rho$ result in a larger
test statistic.

In the following example, we keep the random part of the relationship relatively
small. So we are drawing from a population with a strong negative linear
relationship. We will use varying sample sizes (from small to large).

```{r}
# Create a sample dataset
n <- 3
dat <- tibble(X = runif(n, min = 20, max = 40))
dat <- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 1))
```

```{r}
# Visualize the relationships
ggplot(dat, aes(x = X)) +
  geom_point(aes(y = Y1), color = "darkred") +
  geom_abline(intercept = -5, slope = -5, color = "darkred") +
  theme_classic()
```

```{r}
cor.test(x = dat$X, y = dat$Y1, method = "pearson")
```

```{r}
# Create a sample dataset
n <- 30
dat <- tibble(X = runif(n, min = 20, max = 40))
dat <- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 1))
```

```{r}
# Visualize the relationships
ggplot(dat, aes(x = X)) +
  geom_point(aes(y = Y1), color = "darkred") +
  geom_abline(intercept = -5, slope = -5, color = "darkred") +
  theme_classic()
```

```{r}
cor.test(x = dat$X, y = dat$Y1, method = "pearson")
```

```{r}
# Create a sample dataset
n <- 50
dat <- tibble(X = runif(n, min = 20, max = 40))
dat <- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 1))
```

```{r}
# Visualize the relationships
ggplot(dat, aes(x = X)) +
  geom_point(aes(y = Y1), color = "darkred") +
  geom_abline(intercept = -5, slope = -5, color = "darkred") +
  theme_classic()
```

```{r}
cor.test(x = dat$X, y = dat$Y1, method = "pearson")
```

We will now increae the variability around the regression line of the linear
relationship.

```{r}
# Create a sample dataset
n <- 3
dat <- tibble(X = runif(n, min = 20, max = 40))
dat <- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 30))
```

```{r}
# Visualize the relationships
ggplot(dat, aes(x = X)) +
  geom_point(aes(y = Y1), color = "darkred") +
  geom_abline(intercept = -5, slope = -5, color = "darkred") +
  theme_classic()
```

```{r}
cor.test(x = dat$X, y = dat$Y1, method = "pearson")
```

```{r}
# Create a sample dataset
n <- 30
dat <- tibble(X = runif(n, min = 20, max = 40))
dat <- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 30))
```

```{r}
# Visualize the relationships
ggplot(dat, aes(x = X)) +
  geom_point(aes(y = Y1), color = "darkred") +
  geom_abline(intercept = -5, slope = -5, color = "darkred") +
  theme_classic()
```

```{r}
cor.test(x = dat$X, y = dat$Y1, method = "pearson")
```

```{r}
# Create a sample dataset
n <- 50
dat <- tibble(X = runif(n, min = 20, max = 40))
dat <- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 30))
```

```{r}
# Visualize the relationships
ggplot(dat, aes(x = X)) +
  geom_point(aes(y = Y1), color = "darkred") +
  geom_abline(intercept = -5, slope = -5, color = "darkred") +
  theme_classic()
```

```{r}
cor.test(x = dat$X, y = dat$Y1, method = "pearson")
```

### No Relationship

Assume there is no relationship between $X$ and $Y$, then the true Pearson
correlation coefficient $\rho = 0$. Therefore, the null hypothesis should not
be rejected.

```{r}
# Create a sample dataset
dat <- tibble(X = runif(100, min = 20, max = 40), Y = rnorm(100, mean = -5, sd = 1))
```

```{r}
# Visualize the relationships
ggplot(dat, aes(x = X, y = Y)) +
  geom_point(color = "darkred") +
  geom_smooth(method = "loess", formula = "y ~ x", se = F) +
  theme_classic()
```

```{r}
cor.test(x = dat$X, y = dat$Y, method = "pearson")
```

Another example.

```{r}
# Create a sample dataset
dat <- tibble(X = rnorm(100), Y = rnorm(100))
```

```{r}
# Visualize the relationships
ggplot(dat, aes(x = X, y = Y)) +
  geom_point(color = "darkred") +
  geom_smooth(method = "loess", formula = "y ~ x", se = F) +
  theme_classic()
```

```{r}
cor.test(x = dat$X, y = dat$Y, method = "pearson")
```

### Orange Dataset

If we go back to our case study from the trees dataset.

```{r}
ggplot(Orange, aes(x = age, y = circumference)) +
  geom_point() +
  geom_smooth(method = "loess", formula = "y ~ x", se = FALSE) +
  labs(x = "Age (days)", y = "Circumference (mm)") +
  theme_classic()
```

We already described this relationship. We can now calculate the Pearson
correlation coefficient manually. First, we must calculate the covariance. Let
the random variable $X$ denote the age and $Y$ the circumference.

```{r}
# Calculate the quantities required to calculate the covariance
orange_rel <- as_tibble(Orange) %>%
  select(X = age, Y = circumference) %>%
  mutate(X_bar = mean(X), Y_bar = mean(Y)) %>%
  mutate(X_minus_X_bar = X - X_bar, Y_minus_Y_bar = Y - Y_bar) %>%
  mutate(cross_product = X_minus_X_bar * Y_minus_Y_bar)

kable(orange_rel)
```

We can now calculate the covariance as follows:

$$
Cov(X, Y) = \frac{\sum_{i = 1}^{n} (X_i - \overline{X})(Y_i - \overline{Y})}{n - 1}
$$

Which gives us:

```{r}
sum(orange_rel$cross_product) / (nrow(orange_rel) - 1)
```

or:

```{r}
cov(orange_rel$X, orange_rel$Y)
```

We can now divide the covariance between $X$ and $Y$ by the product of the
sample standard deviations of $X$ and $Y$:

```{r}
(sum(orange_rel$cross_product) / (nrow(orange_rel) - 1)) / 
  (sd(orange_rel$X) * sd(orange_rel$Y))
```

or:

```{r}
cor(x = orange_rel$X, y = orange_rel$Y, method = "pearson")
```

Before interpreting the estimate of the Pearson correlation coefficient, we can
perform a hypothesis test to check if it differs from 0 at a significance level
of $\alpha = 0.05$. The null and alternative hypotheses state:

$$
H_0 : \rho = 0 \text{ vs. } H_a : \rho \neq 0
$$

We perform the hypothesis test:

```{r}
cor.test(x = Orange$age, y = Orange$circumference, method = "pearson")
```

To conclude, we can state that the visualization of the observations and a
LOESS curve indicate a positive linear relationship between age and
circumference of trees. The Pearson correlation coefficient estimate of
$r = 0.9135189$ ($p < 0.0001$) indicates a very strong positive linear relation
between age and circumference.

## Spearman Rank Correlation

The Spearman rank correlation coefficient can be used for ordinal or rank-ordered
data. To calculate the Spearman correlation for variables $X$ and $Y$, we first
must obtain the rank for every value per variable. For this, we order the values
from large to small and assign a rank to each value.

Let the random variable $X$ denote the age in days and $Y$ denote the
circumference in millimeters. Let $R_{xi}$ denote the rank for the observed
value $x_i$ and $R_{yi}$ the rank for the observed value $y_i$.

```{r}
orange_spm <- as_tibble(Orange) %>%
  select(X = age, Y = circumference) %>%
  mutate(X_rank = rank(X), Y_rank = rank(Y))

kable(orange_spm)
```

With the ranks, we can then calculate the difference in ranks for every
observation as follows:

$$
d_i = R_{xi} - R_{yi}
$$

Subsequently, we can calculate the sample Spearman correlation:

$$
r_s = 1 - \frac{6 \sum_{i = 1}^{n} d_{i}^{2}}{n(n^2 - 1)}
$$

We can perform a hypothesis test with the following null and alternative
hypotheses:

$$
H_0 : \rho = 0 \text{ vs. } H_a : \rho \neq 0
$$

Under the null hypothesis and if the sample size is large enough ($n \geq 10$),
the Spearman rank correlation coefficient is approximately normally distributed
with $\mu_{r_s} = 0$ and $\sigma_{r_s} = \sqrt{\frac{1}{n - 1}}$. Using this,
we can calculate the test statistic $T$:

$$
T = \frac{r_s - \mu_{r_s}}{\sigma_{r_s}}
$$

We can continue with the **Orange** dataset:

```{r}
orange_spm <- orange_spm %>%
  mutate(delta = X_rank - Y_rank, delta_2 = delta^2)

kable(orange_spm)
```

We now calculate the sample Spearman rank correlation coefficient:

```{r}
n <- nrow(orange_spm)
1 - ((6 * sum(orange_spm$delta_2)) / (n * (n^2 - 1)))
```

or:

```{r}
cor(x = orange_spm$X, y = orange_spm$Y, method = "spearman")
```

We can now continue and calculate the test statistic $T$. First, we must
calculate the parameters of the null distribution:

$$
\mu_{r_s} = 0
$$

and:

$$
\sigma_{r_s} = \sqrt{\frac{1}{n - 1}} = \sqrt{\frac{1}{35 - 1}} = 0.1714986
$$

Our test statistic $T$ is:

$$
T = \frac{r_s - \mu_{r_s}}{\sigma_{r_s}} = \frac{0.9073529 - 0}{0.1714986} = 5.290731
$$

We calculate the $p$-value as follows:

```{r}
# Calculate the statistics for the null distribution
T_mu <- 0
T_se <- sqrt(1 / (35 - 1))
T_stat <- (0.9073529 - T_mu) / T_se

p_val <- pnorm(q = T_stat, lower.tail = FALSE)*2
p_val
```

Or we can also calculate as follows:

```{r}
# Calculate the test statistic
cor.test(x = orange_spm$X, y = orange_spm$Y, method = "spearman", exact = F)
```

Using a sample dataset:

```{r}
# Create a sample dataset
dat <- tibble(X = rnorm(15), Y = rnorm(15))
```

```{r}
dat
```

```{r}
dat <- dat %>%
  mutate(X_rank = rank(X), Y_rank = rank(Y)) %>%
  mutate(delta = X_rank - Y_rank, delta_2 = delta^2)
```

```{r}
n <- nrow(dat)
1 - ((6 * sum(dat$delta_2)) / (n * (n^2 - 1)))
```

or:

```{r}
cor(x = dat$X, y = dat$Y, method = "spearman")
```

```{r}
# Calculate the statistics for the null distribution
T_mu <- 0
T_se <- sqrt(1 / (nrow(dat) - 1))
T_stat <- (0.15 - T_mu) / T_se

p_val <- pnorm(q = T_stat, lower.tail = FALSE)*2
p_val
```

```{r}
# Calculate the test statistic
cor.test(x = dat$X, y = dat$Y, method = "spearman", exact = F)
```
