[["relationships-and-regression.html", "5 Relationships and Regression 5.1 Relationships 5.2 Pearson Correlation Coefficient", " 5 Relationships and Regression 5.1 Relationships Two random variables are related when the changes in one variable are consistently associated with the changes in another variable. Or, in other words, when the two random variables tend to change together. We say that the two variables have a relationship (or relation). As an example, let us look at the trees dataset and visualize the relationship between height and volume of trees. ggplot(trees, aes(x = Height, y = Volume)) + geom_point() + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;, se = FALSE) + theme_classic() 5.2 Pearson Correlation Coefficient The Pearson correlation coefficient is a statistical measure that quantifies the direction and strength of a linear relationship between two variables. The sign of the Pearson correlation coefficient indicates the direction and the absolute value, ranging from \\(0\\) to \\(1\\), indicates the strength of the relationship. 5.2.1 Calculating Coefficient The Pearson correlation coefficient is calculated as follows: \\[ r = \\frac{Cov(X, Y)}{\\sigma_{X} \\sigma_{Y}} = \\frac{\\sigma_{XY}}{\\sigma_{X} \\sigma_{Y}} \\] Dividing the covariance between \\(X\\) and \\(Y\\) by the product of the standard deviations of \\(X\\) and \\(Y\\) ensures that the correlation coefficient is not dependent on the scale of the variables \\(X\\) and \\(Y\\) and that \\(r \\in [-1, +1]\\). If \\(r = +1\\), then all of the data falls on a single straight line with a positive slope. Some examples: # Create a sample dataset dat &lt;- tibble(X = runif(n = 20, min = 10, max = 80)) dat &lt;- add_column(dat, Y1 = 0.30*dat$X) dat &lt;- add_column(dat, Y2 = 3.00*dat$X) dat &lt;- add_column(dat, Y3 = 30.0*dat$X) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y1), formula = &quot;y ~ x&quot;, color = &quot;darkred&quot;, se = F) + geom_point(aes(y = Y2), color = &quot;darkgreen&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y2), formula = &quot;y ~ x&quot;, color = &quot;darkgreen&quot;, se = F) + geom_point(aes(y = Y3), color = &quot;darkblue&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y3), formula = &quot;y ~ x&quot;, color = &quot;darkblue&quot;, se = F) + theme_classic() Note that these examples illustrate a functional relationship. The relationship between \\(X\\) and each of the \\(Y\\) variables (\\(Y1\\), \\(Y2\\), \\(Y3\\)) can be described by an equation and all the observations fall on the line of the relationship. If we compute the Pearson correlation coefficients for each of these three pairs of variables, we see that it is each time equal to \\(r = +1\\). cor(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## [1] 1 cor(x = dat$X, y = dat$Y2, method = &quot;pearson&quot;) ## [1] 1 cor(x = dat$X, y = dat$Y3, method = &quot;pearson&quot;) ## [1] 1 If we now use a negative slope, we will see that the Pearson correlation coefficient for each of these functional relationships equals \\(r = -1\\). # Create a sample dataset dat &lt;- tibble(X = runif(n = 20, min = 10, max = 80)) dat &lt;- add_column(dat, Y1 = -0.30*dat$X) dat &lt;- add_column(dat, Y2 = -3.00*dat$X) dat &lt;- add_column(dat, Y3 = -30.0*dat$X) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y1), formula = &quot;y ~ x&quot;, color = &quot;darkred&quot;, se = F) + geom_point(aes(y = Y2), color = &quot;darkgreen&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y2), formula = &quot;y ~ x&quot;, color = &quot;darkgreen&quot;, se = F) + geom_point(aes(y = Y3), color = &quot;darkblue&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y3), formula = &quot;y ~ x&quot;, color = &quot;darkblue&quot;, se = F) + theme_classic() cor(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## [1] -1 cor(x = dat$X, y = dat$Y2, method = &quot;pearson&quot;) ## [1] -1 cor(x = dat$X, y = dat$Y3, method = &quot;pearson&quot;) ## [1] -1 We see that the absolute value of the Pearson correlation coefficient equals \\(1\\) for the six examples. This indicates that these are very strong linear relationships. But what does it mean for a relationship between two variables to be strong? The strength of a relationship between two variables is an indication of how much information a given value for one variables gives us about the value of the other variable. If we see that for a given value of \\(X\\) the range of possible or observed values for \\(Y\\) is relatively large, and this for every level of \\(X\\), then we cannot say that \\(X\\) carries much information about \\(Y\\). If, however, the range of observed values for \\(Y\\) is relatively small for every level of \\(X\\), then we can state that \\(X\\) carries a lot of information about the value of \\(Y\\) and the relationship is quite strong. We will showcase with a few examples. # Create a sample dataset dat &lt;- tibble(X = runif(100, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 + 5*dat$X + rnorm(100, sd = 1)) dat &lt;- add_column(dat, Y2 = -5 + 5*dat$X + rnorm(100, sd = 10)) dat &lt;- add_column(dat, Y3 = -5 + 5*dat$X + rnorm(100, sd = 25)) For the following three positive linear relationships, we see that the amount of scatter around the regression line is different for the three examples. The amount of scatter around the regression line is an indication of how much information the variable \\(X\\) carries about the variable \\(Y\\). If we look at \\(X = 35\\), then we see that for \\(Y1\\), the range of observed values for \\(Y1\\) is relatively small. For \\(Y2\\), the range of observed \\(Y2\\) values is larger. And for \\(Y3\\), the range of observed \\(Y3\\) values is relatively large. Therefore, we can state that the relationship between \\(X\\) and \\(Y1\\) is strong, between \\(X\\) and \\(Y2\\) is moderate, and between \\(X\\) and \\(Y3\\) is weak. # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = 5, color = &quot;darkred&quot;) + theme_classic() # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y2), color = &quot;darkgreen&quot;) + geom_abline(intercept = -5, slope = 5, color = &quot;darkgreen&quot;) + theme_classic() # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y3), color = &quot;darkblue&quot;) + geom_abline(intercept = -5, slope = 5, color = &quot;darkblue&quot;) + theme_classic() The absolute values of the Pearson correlation coefficient are indicative of the strength of the linear relationship: cor(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## [1] 0.9994782 cor(x = dat$X, y = dat$Y2, method = &quot;pearson&quot;) ## [1] 0.9359107 cor(x = dat$X, y = dat$Y3, method = &quot;pearson&quot;) ## [1] 0.7906265 5.2.2 Hypothesis Testing All of the above examples on the Pearson correlation coefficient made use of samples of data to calculate the correlation. When analyzing data, we must be be aware that often we are analyzing a sample of data which is drawn from a larger population. For this population, there is a true Pearson correlation coefficient \\(\\rho\\), which is an unknown constant. To estimate \\(\\rho\\), we draw a random sample from the population and use an estimator to produce an estimate \\(\\hat{\\rho}\\). We have already seen the estimator in this chapter. It is: \\[ r = \\frac{Cov(X, Y)}{\\sigma_{X} \\sigma_{Y}} = \\frac{\\sigma_{XY}}{\\sigma_{X} \\sigma_{Y}} \\] We must remember that the Pearson correlation coefficient \\(r\\) is an estimator for the true Pearson correlation coefficient \\(\\rho\\) in the population. The latter is an unknown constant. We want to estimate this unknown constant and therefore we sample data from the population and analyze this data to produce an estimate of the true value of \\(\\rho\\). Using this, we can perform a \\(t\\)-test to test for the existence of a linear relationship between two variables. The hypotheses for this \\(t\\)-test state: \\[ H_0 : \\rho = 0 \\] and: \\[ H_a : \\rho \\neq 0 \\] The test statistic \\(T\\) for this test is: \\[ T = \\frac{r \\cdot \\sqrt{n - 2}}{\\sqrt{1 - R^2}} \\] The test statistic \\(T\\) follows a \\(t\\)-distribution with \\(n - 2\\) degrees of freedom. We can also perform an exact hypothesis test. cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;, exact = TRUE) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = 306.31, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9992231 0.9996495 ## sample estimates: ## cor ## 0.9994782 cor.test(x = dat$X, y = dat$Y2, method = &quot;pearson&quot;, exact = TRUE) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y2 ## t = 26.303, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9060519 0.9564963 ## sample estimates: ## cor ## 0.9359107 cor.test(x = dat$X, y = dat$Y3, method = &quot;pearson&quot;, exact = TRUE) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y3 ## t = 12.783, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.7034495 0.8543671 ## sample estimates: ## cor ## 0.7906265 In these three examples, we see that the \\(p\\)-values are all statistically significant at \\(\\alpha = 0.05\\). If we look at the test statistic \\(T\\), we see that both sample size \\(n\\) and the estimate \\(r\\) affect the test statistic. We see that larger sample sizes and larger estimates of \\(\\rho\\) result in a larger test statistic. In the following example, we keep the random part of the relationship relatively small. So we are drawing from a population with a strong negative linear relationship. We will use varying sample sizes (from small to) # Create a sample dataset n &lt;- 3 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 1)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;, exact = TRUE) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -24.093, df = 1, p-value = 0.02641 ## alternative hypothesis: true correlation is not equal to 0 ## sample estimates: ## cor ## -0.9991397 # Create a sample dataset n &lt;- 30 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 1)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;, exact = TRUE) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -156.16, df = 28, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9997302 -0.9987807 ## sample estimates: ## cor ## -0.9994264 # Create a sample dataset n &lt;- 50 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 1)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;, exact = TRUE) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -197.94, df = 48, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9996545 -0.9989161 ## sample estimates: ## cor ## -0.999388 We will now increae the variability around the regression line of the linear relationship. # Create a sample dataset n &lt;- 3 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 30)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;, exact = TRUE) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -2.2476, df = 1, p-value = 0.2665 ## alternative hypothesis: true correlation is not equal to 0 ## sample estimates: ## cor ## -0.9136472 # Create a sample dataset n &lt;- 30 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 30)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;, exact = TRUE) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -4.305, df = 28, p-value = 0.0001846 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.8077183 -0.3505224 ## sample estimates: ## cor ## -0.6310953 # Create a sample dataset n &lt;- 50 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 30)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;, exact = TRUE) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -5.8414, df = 48, p-value = 4.376e-07 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.7825385 -0.4463282 ## sample estimates: ## cor ## -0.644595 5.2.3 No Relationship Assume there is no relationship between \\(X\\) and \\(Y\\), then the true Pearson correlation coefficient \\(\\rho = 0\\). Therefore, the null hypothesis should not be rejected. # Create a sample dataset dat &lt;- tibble(X = runif(100, min = 20, max = 40)) dat &lt;- add_column(dat, Y = -5 + rnorm(100, sd = 1)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y), color = &quot;darkred&quot;) + geom_hline(yintercept = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y, method = &quot;pearson&quot;, exact = TRUE) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y ## t = 1.7904, df = 98, p-value = 0.07647 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.01911189 0.36174673 ## sample estimates: ## cor ## 0.1779743 Another example. # Create a sample dataset dat &lt;- tibble(X = rnorm(100), Y = rnorm(100)) # Visualize the relationships ggplot(dat, aes(x = X, y = Y)) + geom_point(color = &quot;darkred&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;,, se = F) + theme_classic() cor.test(x = dat$X, y = dat$Y, method = &quot;pearson&quot;, exact = TRUE) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y ## t = -0.8925, df = 98, p-value = 0.3743 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.2812493 0.1085409 ## sample estimates: ## cor ## -0.08979147 5.2.4 Case Study If we go back to our case study from the trees dataset. ggplot(trees, aes(x = Height, y = Volume)) + geom_point() + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;, se = FALSE) + theme_classic() We can see a positive linear trend. We calculate an estimate of the true Pearson correlation coefficient and test for: \\[ H_0 : \\rho = 0 \\text{ vs. } H_a : \\rho \\neq 0 \\] cor.test(x = trees$Height, y = trees$Volume, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: trees$Height and trees$Volume ## t = 4.0205, df = 29, p-value = 0.0003784 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3095235 0.7859756 ## sample estimates: ## cor ## 0.5982497 There seems to be a positive linear relationship between the height and the volume of trees (\\(r = 0.598\\), \\(p &lt; 0.001\\)). Remember: The sign of the Pearson correlation coefficient informs us about the direction of the linear relationship, while the absolute value informs us about the strength of the linear relationship. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
