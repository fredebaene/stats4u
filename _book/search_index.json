[["index.html", "Stats For You 1 About", " Stats For You Frederick De Baene 2026-01-02 1 About Stats For You (stats4u) provides an introduction to and examples on various topics encountered within statistics and probability. "],["logistic-models-and-deviance.html", "2 Logistic Models and Deviance 2.1 Models 2.2 Likelihood and Log-Likelihood 2.3 Deviance", " 2 Logistic Models and Deviance When fitting a logistic regression model to a data set, R outputs several key metrics, including the null deviance and the residual deviance, along with their respective degrees of freedom. This chapter aims to clarify the calculation methods and meaning of these deviance values. We will use the first 20 observations from a heart disease data set, fit a logistic regression model to this data, and manually compute the deviance values to illustrate how they are actually calculated. # Read the data and only retain the first 20 observations heart &lt;- readr::read_csv(file = &quot;data/heart.xls&quot;, show_col_types = FALSE) heart &lt;- head(heart, 20) 2.1 Models To understand what these deviance values are, we must first understand the following three models: the null model (\\(M_0\\)), the proposed model (\\(M_p\\)), and the saturated model (\\(M_s\\)). These three models are used to calculate the two deviance values outputted by R, i.e., the null deviance (the deviance of \\(M_0\\)) and the residual deviance(the deviance of \\(M_p\\)). 2.1.1 Proposed Model The proposed model \\(M_p\\) is the model of interest. In our example, it is used to estimate the probability of heart disease for each patient, conditioning on that patient’s age. The model is represented as follows: \\[ logit(\\pi_i) = \\beta_0 + \\beta_1 \\times Age_i + \\epsilon_i \\] with \\(\\pi_i = P(HD_i = 1 | Age_i)\\) the conditional probability of a heart disease for patient \\(i\\), \\(Age_i\\) the age of patient \\(i\\), and \\(\\epsilon_i\\) the random error of the model for the \\(i\\)’th observation. We can fit the proposed model as follows: fit_p &lt;- glm( formula = HeartDisease ~ Age, data = heart, family = binomial(link = &quot;logit&quot;) ) ## ## Call: ## glm(formula = HeartDisease ~ Age, family = binomial(link = &quot;logit&quot;), ## data = heart) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.48884 2.87127 -0.867 0.386 ## Age 0.04571 0.06191 0.738 0.460 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 26.920 on 19 degrees of freedom ## Residual deviance: 26.365 on 18 degrees of freedom ## AIC: 30.365 ## ## Number of Fisher Scoring iterations: 4 We obtain the null deviance \\(D_0 = 26.9\\) and the residual deviance \\(D_p = 26.4\\). 2.1.2 Null Model The null model \\(M_0\\) is a model without any predictors. It only contains an intercept \\(\\beta_0\\): \\[ logit(\\pi_i) = \\beta_0 + \\epsilon_i \\] The estimated probability of heart disease is the same for each patient. The null model \\(M_0\\) is fitted as follows: fit_0 &lt;- glm( formula = HeartDisease ~ 1, data = heart, family = binomial(link = &quot;logit&quot;) ) ## ## Call: ## glm(formula = HeartDisease ~ 1, family = binomial(link = &quot;logit&quot;), ## data = heart) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.4055 0.4564 -0.888 0.374 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 26.92 on 19 degrees of freedom ## Residual deviance: 26.92 on 19 degrees of freedom ## AIC: 28.92 ## ## Number of Fisher Scoring iterations: 4 R outputs the null deviance \\(D_0\\) and the residual deviance \\(D_p\\), which both equal 26.9. They are the same because \\(M_p\\) is the same as \\(M_0\\) (neither of them contains a predictor, only an intercept). 2.1.3 Saturated Model The saturated model \\(M_s\\) is a model that perfectly fits data. This model perfectly predicts the outcome for each patient in the data set. If patient \\(i\\) has a heart disease (\\(HD_i = 1\\)), then \\(\\hat{\\pi_i} = 1\\). If patient \\(i\\) does not have a heart disease (\\(HD_i = 0\\)), then \\(\\hat{\\pi_i} = 0\\). We will not fit a saturated model \\(M_s\\) as such to the data, but continue our explanation knowing the fact that the model perfectly predicts the outcome for each observation \\(i\\). 2.1.4 Models and Deviance Since the saturated model \\(M_s\\) perfectly fits the data, it serves as our reference point for assessing goodness-of-fit. The proposed model \\(M_p\\) and the null model \\(M_0\\) are compared to the saturated model in terms of their goodness-of-fit. The result of these comparisons is a deviance value for each model. The deviance quantifies the degree to which the null model and the proposed model deviate from the saturated model in terms of goodness-of-fit. To calculate the actual deviance, we utilize the likelihoods and log-likelihoods of the models (see below for further details). 2.2 Likelihood and Log-Likelihood The likelihood is a probability that indicates how likely it is that the parameters take on particular values given a particular sample. As the sample is given, the values for the outcome and predictor are not variables. The parameter values are the variables. We can represent the likelihood function as follows: \\[ L(\\theta | (x_1, y_1), (x_2, y_2), ..., (x_n, y_n)) = \\prod_{i = 1}^{n} f_Y(y_i; x_i, \\theta) \\] with \\(\\theta\\) the parameters of interest, \\(n\\) the sample size, and \\(f_Y(y; x, \\theta)\\) the probability mass function for the response. Calculating the likelihood involves taking the product of many probabilities (i.e., values between 0 and 1), and therefore results in a very small value. For computational reasons, often the log-likelihood is calculated instead of the likelihood. \\[ \\begin{aligned} l(\\theta | (x_1, y_1), (x_2, y_2), ..., (x_n, y_n)) &amp;= log[\\prod_{i = 1}^{n} f_Y(y_i; x_i, \\theta)] \\\\ &amp;= \\sum_{i = 1}^{n} log(f_Y(y_i; x_i, \\theta)) \\end{aligned} \\] 2.2.1 Proposed Model We start by calculating the likelihood and log-likelihood of \\(M_p\\). \\[ L(\\theta | (x_1, y_1), (x_2, y_2), ..., (x_n, y_n)) = \\prod_{i = 1}^{n} f_Y(y_i; x_i, \\theta) \\] with \\(\\theta\\) representing the parameters that are the variables and \\(n\\) the sample size. To define the likelihood function, we must first determine the probability mass function for the response. The PMF defines the probability distribution of a discrete random variable. Remember that a logistic regression model is used to predict the outcome for a discrete random variable, and more specifically a binary random variable that follows a Bernoulli distribution: \\[ HD \\sim Bernoulli(\\pi) \\] The probability mass function (PMF) of a Bernoulli distribution is the following: \\[ f_Y(y; \\pi) = \\pi^{y} \\times (1 - \\pi)^{1 - y} \\] Therefore, we use the PMF of the Bernoulli distribution for the likelihood function: \\[ L(\\theta | (x_1, y_1), (x_2, y_2), ..., (x_n, y_n)) = \\prod_{i = 1}^{n} \\pi_{i}^{y_i} \\times (1 - \\pi_i)^{1 - y_i} \\] with \\(\\theta\\) representing the regression coefficients of the logistic regression model. We can now also define the log-likelihood function: \\[ \\begin{aligned} l(\\theta | (x_1, y_1), (x_2, y_2), ..., (x_n, y_n)) &amp;= log[\\prod_{i = 1}^{n} \\pi_{i}^{y_i} \\times (1 - \\pi_i)^{1 - y_i}] \\\\ &amp;= \\sum_{i = 1}^{n} [y_i \\times log(\\pi_i) + (1 - y_i) \\times log(1 - \\pi_i)] \\end{aligned} \\] Using our fitted model \\(M_p\\), how can we now obtain the log-likelihood? We start by obtaining the estimated probability of observing the actual outcome for each observation in the data set. We focus on the third observation in the data set. We see that \\(Age_3 = 37\\) and \\(HD_3 = 0\\). We can use our model to obtain the probability of a heart disease: # Estimate the probability of a heart disease for i = 3 hd_prob_003 &lt;- predict(fit_p, type = &quot;response&quot;)[3] hd_prob_003 ## 3 ## 0.3105577 We see that this probability is \\(P(HD_3 = 1 | Age_i = 37) = \\pi_i = 0.31\\). Assume that our sample only comprises this one patient, then the likelihood would be: \\[ \\begin{aligned} L(\\theta | (Age_i, HD_i)) &amp;= \\prod_{i = 1}^{n} f_Y(y_i; x_i, \\theta) \\\\ &amp;= \\prod_{i = 1}^{n} \\pi_{i}^{y_i} \\times (1 - \\pi_i)^{1 - y_i} \\\\ &amp;= 0.3105577^{0} \\times (1 - 0.3105577)^{1 - 0} \\\\ &amp;= 1 \\times 0.6894423^{1} \\\\ &amp;= 0.6894423 \\end{aligned} \\] So, to calculate the likelihood, first, we calculate the estimated probability with which the actual outcome occurs. # Create a copy of the data frame as to not overwrite the original data heart_p &lt;- heart # Calculate the probability of a patient having or not having a heart disease # using the proposed model (i.e., given the patient&#39;s age) heart_p$EstProb_HD_1 &lt;- predict(fit_p, type = &quot;response&quot;) heart_p$EstProb_HD_0 &lt;- 1 - predict(fit_p, type = &quot;response&quot;) heart_p &lt;- heart_p %&gt;% dplyr::mutate( ProbOutcome = EstProb_HD_1^HeartDisease * EstProb_HD_0^(1 - HeartDisease), LogProbOutcome = log(ProbOutcome) ) Age Heart Disease P(HD = 1) P(HD = 0) P(Outcome) Log(P(Outcome)) 40 0 0.3406546 0.6593454 0.6593454 -0.4165078 49 1 0.4380759 0.5619241 0.4380759 -0.8253630 37 0 0.3105577 0.6894423 0.6894423 -0.3718723 48 1 0.4268570 0.5731430 0.4268570 -0.8513061 54 0 0.4948960 0.5051040 0.5051040 -0.6829909 39 0 0.3304634 0.6695366 0.6695366 -0.4011694 45 0 0.3936917 0.6063083 0.6063083 -0.5003666 54 0 0.4948960 0.5051040 0.5051040 -0.6829909 37 1 0.3105577 0.6894423 0.3105577 -1.1693854 48 0 0.4268570 0.5731430 0.5731430 -0.5566201 37 0 0.3105577 0.6894423 0.6894423 -0.3718723 58 1 0.5405183 0.4594817 0.5405183 -0.6152267 39 0 0.3304634 0.6695366 0.6695366 -0.4011694 49 1 0.4380759 0.5619241 0.4380759 -0.8253630 42 0 0.3614779 0.6385221 0.6385221 -0.4485990 54 0 0.4948960 0.5051040 0.5051040 -0.6829909 38 1 0.3204289 0.6795711 0.3204289 -1.1380949 43 0 0.3720940 0.6279060 0.6279060 -0.4653649 60 1 0.5631245 0.4368755 0.5631245 -0.5742545 36 1 0.3008561 0.6991439 0.3008561 -1.2011233 The above data show, given the fitted model, the probability with which each actual outcome occurs, given the patient’s age, and also the log-transformed probability. To obtain the log-likelihood of the fitted model \\(LL_p\\), we can now just sum these log-transformed probabilities: # Obtain the log-likelihood for the proposed model ll_p &lt;- sum(heart_p$LogProbOutcome) ll_p ## [1] -13.18263 2.2.2 Null Model We can also do this for the null model. Remember that the fitted probability of a heart disease will be the same for each patient, as we do not take into account any patient information. We can then compute the log-likelihood of the null model as follows: # Create a copy of the data frame as to not overwrite the original data heart_0 &lt;- heart # Calculate the probability of a patient having or not having a heart disease # using the null model (i.e., not taking into account any patient information) heart_0$EstProb_HD_1 &lt;- predict(fit_0, type = &quot;response&quot;) heart_0$EstProb_HD_0 &lt;- 1 - predict(fit_0, type = &quot;response&quot;) heart_0 &lt;- heart_0 %&gt;% dplyr::mutate( ProbOutcome = EstProb_HD_1^HeartDisease * EstProb_HD_0^(1 - HeartDisease), LogProbOutcome = log(ProbOutcome) ) Age Heart Disease P(HD = 1) P(HD = 0) P(Outcome) Log(P(Outcome)) 40 0 0.4 0.6 0.6 -0.5108256 49 1 0.4 0.6 0.4 -0.9162907 37 0 0.4 0.6 0.6 -0.5108256 48 1 0.4 0.6 0.4 -0.9162907 54 0 0.4 0.6 0.6 -0.5108256 39 0 0.4 0.6 0.6 -0.5108256 45 0 0.4 0.6 0.6 -0.5108256 54 0 0.4 0.6 0.6 -0.5108256 37 1 0.4 0.6 0.4 -0.9162907 48 0 0.4 0.6 0.6 -0.5108256 37 0 0.4 0.6 0.6 -0.5108256 58 1 0.4 0.6 0.4 -0.9162907 39 0 0.4 0.6 0.6 -0.5108256 49 1 0.4 0.6 0.4 -0.9162907 42 0 0.4 0.6 0.6 -0.5108256 54 0 0.4 0.6 0.6 -0.5108256 38 1 0.4 0.6 0.4 -0.9162907 43 0 0.4 0.6 0.6 -0.5108256 60 1 0.4 0.6 0.4 -0.9162907 36 1 0.4 0.6 0.4 -0.9162907 We obtain the log-likelihood of the null model \\(LL_0\\) as follows: ll_0 &lt;- sum(heart_0$LogProbOutcome) ll_0 ## [1] -13.46023 2.2.3 Saturated Model We can follow the same procedure for \\(M_s\\). The saturated model perfectly fits the data, so the estimated probability of heart disease reflects the actual outcome. In other words, if \\(HD_i = 1\\), then \\(P(HD_i = 1) = 1\\). And if \\(HD_i = 0\\), then \\(P(HD_i = 1) = 0\\). # Create a copy of the data frame as to not overwrite the original data heart_s &lt;- heart # Calculate the probability of a patient having or not having a heart disease # using the saturated model (i.e., the model perfectly fits the data) heart_s$EstProb_HD_1 &lt;- heart_s$HeartDisease heart_s$EstProb_HD_0 &lt;- 1 - heart_s$HeartDisease heart_s &lt;- heart_s %&gt;% dplyr::mutate( ProbOutcome = EstProb_HD_1^HeartDisease * EstProb_HD_0^(1 - HeartDisease), LogProbOutcome = log(ProbOutcome) ) Age Heart Disease P(HD = 1) P(HD = 0) P(Outcome) Log(P(Outcome)) 40 0 0 1 1 0 49 1 1 0 1 0 37 0 0 1 1 0 48 1 1 0 1 0 54 0 0 1 1 0 39 0 0 1 1 0 45 0 0 1 1 0 54 0 0 1 1 0 37 1 1 0 1 0 48 0 0 1 1 0 37 0 0 1 1 0 58 1 1 0 1 0 39 0 0 1 1 0 49 1 1 0 1 0 42 0 0 1 1 0 54 0 0 1 1 0 38 1 1 0 1 0 43 0 0 1 1 0 60 1 1 0 1 0 36 1 1 0 1 0 Finally, we obtain the log-likelihood for the saturated model \\(LL_s\\): ll_s &lt;- sum(heart_s$LogProbOutcome) ll_s ## [1] 0 2.3 Deviance As stated earlier, the saturated model \\(M_s\\) perfectly fits the data, and therefore serves as our reference point for goodness-of-fit. We now use the log-likelihood values of these models to calculate the deviance. For the residual deviance \\(D_p\\), we have: \\[ \\begin{aligned} D_p &amp;= 2 \\times (LL_s - LL_p) \\\\ &amp;= 2 \\times (0 - (-13.18263)) \\\\ &amp;= 2 \\times 13.18263 \\\\ &amp;= 26.36526 \\end{aligned} \\] and for the null deviance \\(D_0\\), we have: \\[ \\begin{aligned} D_0 &amp;= 2 \\times (LL_s - LL_0) \\\\ &amp;= 2 \\times (0 - (-13.46023)) \\\\ &amp;= 2 \\times 13.46023 \\\\ &amp;= 26.92047 \\end{aligned} \\] As we can see, the deviance is two times the difference between the log-likelihood of the saturated model and the log-likelihood of the proposed model for the residual deviance or the log-likelihood of the null model for the null deviance. The deviance quantifies how much the null and proposed models differ from the saturated model in terms of goodness-of-fit. In a following chapter, we will explain how we can use these deviance value to perform statistical hypothesis tests to formally compare the proposed model to the null model. "],["logistic-regression.html", "3 Logistic Regression 3.1 Ordinary Logistic Regression 3.2 Conditional Logistic Regression", " 3 Logistic Regression In this chapter, we explore logistic regression. This type of model can be used to estimate the probability of a particular event occurring or not. This means that the outcome must be binary, i.e., there are only two possible outcomes. 3.1 Ordinary Logistic Regression The most well-known logistic regression is ordinary logistic regression. In a next chapter, we will discuss conditional logistic regression. 3.1.1 Context Suppose we randomly select a patient and assess his/her survival status. The outcome of this random experiment is captured using the random variable \\(Y\\). The random variable \\(Y\\) can take on one of only two possible outcomes, therefore it is a binary random variable (or Bernoulli random variable). If we repeat this process for \\(n\\) patients, then we have \\(n\\) random variables, i.e., \\(Y_i\\) for \\(i = 1, 2, ..., n\\). Assume we observe the following: ## i Y ## 1 1 1 ## 2 2 0 ## 3 3 0 ## 4 4 1 ## 5 5 0 We have \\(n = 5\\) patients. For each patient, the outcome \\(Y_i\\) is observed. Because the variable \\(Y_i\\) is a Bernoulli random variable, its probability distribution is defined by the following probability mass function: \\[ f_i (Y_i) = \\pi_i^{Y_i} \\cdot (1 - \\pi_i)^{(1 - Y_i)} \\] We again observe the previously observed outcomes but recognize that, for each patient, the outcome is drawn randomly from a Bernoulli distribution defined by \\(\\pi_i\\): ## i Y P.Y...1. ## 1 1 1 0.2875775 ## 2 2 0 0.7883051 ## 3 3 0 0.4089769 ## 4 4 1 0.8830174 ## 5 5 0 0.9404673 We can also visualize the five Bernoulli distributions for each of the patients. for (idx in seq_along(probs)) { prob &lt;- probs[idx] plot( x = c(0, 1), y = c(1 - prob, prob), xlab = &quot;Y&quot;, ylab = &quot;Probability&quot;, main = paste0(&quot;Patient 0&quot;, idx), ylim = c(0, 1), col = &quot;red&quot;, pch = 16 ) } 3.1.2 Model Fitting We use the heart dataset and denote the outcome in the heart dataset with the variable \\(Y\\), with \\(Y = 1\\) if the patient has a heart disease and \\(Y = 0\\) otherwise. heart$Y &lt;- as.integer(heart$HeartDisease) Prior to fitting a model, we cast the categorical variables to factors. heart$ChestPainType &lt;- factor( x = heart$ChestPainType, levels = c(&quot;ASY&quot;, &quot;NAP&quot;, &quot;TA&quot;, &quot;ATA&quot;) ) heart$Sex &lt;- factor( x = heart$Sex, levels = c(&quot;F&quot;, &quot;M&quot;) ) We fit the following model to the data: \\[ \\mathbb{E} [Y_i] = \\beta_0 + \\beta_1 \\cdot X_{age} + \\beta_2 \\cdot X_{male} \\] with \\(X_{age}\\) the age and \\(X_{male} = 1\\) if the person is male and \\(X_{male} = 0\\) otherwise. The model contains three parameters, i.e., \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\). We call glm() as follows to fit the model: model &lt;- glm( formula = &quot;HeartDisease ~ Age + Sex&quot;, data = heart, family = binomial(link = &quot;logit&quot;) ) ## ## Call: ## glm(formula = &quot;HeartDisease ~ Age + Sex&quot;, family = binomial(link = &quot;logit&quot;), ## data = heart) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.634893 0.481185 -9.632 &lt; 2e-16 *** ## Age 0.066531 0.008165 8.148 3.69e-16 *** ## SexM 1.641195 0.189345 8.668 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1262.1 on 917 degrees of freedom ## Residual deviance: 1101.6 on 915 degrees of freedom ## AIC: 1107.6 ## ## Number of Fisher Scoring iterations: 4 3.1.3 Interpretation Us the parameter estimates, we have the following model: \\[ log(\\frac{\\pi_i}{1 - \\pi_i}) = -4.634893 + 0.066531 \\cdot X_{age} + 1.641195 \\cdot X_{male} \\] Assume that person \\(i\\) is a 40-year-old woman. The probability of person \\(i\\) having a heart disease is the following: \\[ \\begin{align} log(\\frac{\\pi_i}{1 - \\pi_i}) &amp;= -4.634893 + 0.066531 \\cdot 40 + 1.641195 \\cdot 0 \\\\ &amp;= -4.634893 + 0.066531 \\cdot 40 \\\\ &amp;= -1.973653 \\end{align} \\] The estimated logit equals -1.973653. The estimated odds of a 40-year-old woman having a heart disease is 0.1389483. This means that the probability of a 40-year-old woman having a heart disease is 0.1389483 times smaller than the probability of a 40-year-old woman not having a heart disease. \\[ \\begin{align} log(\\frac{\\pi_i}{1 - \\pi_i}) &amp;= -1.973653 \\\\ &amp;\\leftrightarrow \\frac{\\pi_i}{1 - \\pi_i} = exp(-1.973653) \\\\ &amp;\\leftrightarrow \\frac{\\pi_i}{1 - \\pi_i} = 0.1389483 \\end{align} \\] The estimated odds of a 40-year-old woman having a heart disease can be used to estimate the probability of such a person having a heart disease. \\[ \\begin{align} log(\\frac{\\pi_i}{1 - \\pi_i}) &amp;\\leftrightarrow \\pi_i = \\frac{exp(-1.973653)}{1 + exp(-1.973653)} \\\\ &amp;\\leftrightarrow \\pi_i = \\frac{0.1389483}{1 + 0.1389483} \\\\ &amp;\\leftrightarrow \\pi_i = \\frac{0.1389483}{1.1389483} \\\\ &amp;\\leftrightarrow \\pi_i = 0.121997 \\end{align} \\] Assume person \\(j\\) is a 40-year-old man. We can now also calculate the estimated odds of having a heart disease. \\[ \\begin{align} log(\\frac{\\pi_j}{1 - \\pi_j}) &amp;= -4.634893 + 0.066531 \\cdot 40 + 1.641195 \\cdot 1 \\\\ &amp;\\leftrightarrow \\frac{\\pi_j}{1 - \\pi_j} = exp(-4.634893 + 0.066531 \\cdot 40 + 1.641195) \\\\ &amp;\\leftrightarrow \\frac{\\pi_j}{1 - \\pi_j} = exp(-4.634893 + 0.066531 \\cdot 40) \\cdot exp(1.641195) \\\\ &amp;\\leftrightarrow \\frac{\\pi_j}{1 - \\pi_j} = 0.1389483 \\cdot 5.161334 \\\\ &amp;\\leftrightarrow \\frac{\\pi_j}{1 - \\pi_j} = 0.7171586 \\end{align} \\] From this, we can also see the following: \\[ \\begin{align} log(\\frac{\\pi_j}{1 - \\pi_j}) &amp;= -4.634893 + 0.066531 \\cdot 40 + 1.641195 \\cdot 1 \\\\ &amp;\\leftrightarrow \\frac{\\pi_j}{1 - \\pi_j} = exp(-4.634893 + 0.066531 \\cdot 40 + 1.641195) \\\\ &amp;\\leftrightarrow \\frac{\\pi_j}{1 - \\pi_j} = exp(-4.634893 + 0.066531 \\cdot 40) \\cdot exp(1.641195) \\\\ &amp;\\leftrightarrow exp(1.641195) = \\frac{\\frac{\\pi_j}{1 - \\pi_j}}{exp(-4.634893 + 0.066531 \\cdot 40)} \\\\ &amp;\\leftrightarrow exp(1.641195) = \\frac{\\frac{\\pi_j}{1 - \\pi_j}}{\\frac{\\pi_i}{1 - \\pi_i}} \\end{align} \\] In other words, the estimate for \\(\\beta_2\\) can be exponentiated to obtain an odds ratio, and more specifically the odds ratio of the odds of a 40-year-old man having a heart disease vs. a 40-year-old woman having a heart disease. 3.1.4 Maximum Likelihood Estimation The model parameters are estimated using the maximum likelihood estimation framework. We let the random variable \\(Y\\) denote the outcome, with \\(Y = 1\\) indicating that the event of interest occurred and \\(Y = 0\\) that the event of interest did not occur. Because the variable \\(Y\\) is a binary variable its probability distribution can be described using the following probability mass function: \\[ \\mathbb{P} (Y = y) = \\pi^{y} \\cdot (1 - \\pi)^{(1 - y)} \\] with \\(\\pi\\) representing the probability of \\(Y = 1\\). We can verify this: \\[ \\mathbb{P} (Y = 1) = \\pi^{1} \\cdot (1 - \\pi)^{(1 - 1)} = \\pi \\cdot (1 - \\pi)^0 = \\pi \\] and \\[ \\mathbb{P} (Y = 0) = \\pi^0 \\cdot (1 - \\pi)^{(1 - 0)} = (1 - \\pi)^1 = 1 - \\pi \\] This distribution is defined by a single parameter \\(\\pi\\). In the simple logistic regression model, the probability of the event of interest occurring \\(\\pi\\) is linked to the linear predictor as follows: \\[ \\pi_i = \\frac{exp(\\beta_0 + \\beta_1 x_i)}{1 + exp(\\beta_0 + \\beta_1 x_i)} \\] In other words, we have: \\[ log(\\frac{\\pi_i}{1 - \\pi_i}) = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] We can now derive the likelihood and log-likelihood function. The likelihood function is: \\[ \\begin{align} L(\\beta) &amp;= \\prod_{i = 1}^{n} \\pi_i^{y_i} (1 - \\pi_i)^{(1 - y_i)} \\end{align} \\] The log-likelihood function is: \\[ \\begin{align} l(\\beta) = log(L(\\beta)) &amp;= log \\left( \\prod_{i = 1}^{n} \\pi_i^{y_i} (1 - \\pi_i)^{(1 - y_i)} \\right) \\\\ &amp;= \\sum_{i = 1}^{n} \\left[ log(\\pi_i^{y_i} (1 - \\pi_i)^{(1 - y_i)}) \\right] \\\\ &amp;= \\sum_{i = 1}^{n} \\left[ y_i \\cdot log(\\pi_i) + (1 - y_i) \\cdot log(1 - \\pi_i) \\right] \\\\ &amp;= \\sum_{i = 1}^{n} \\left[ y_i \\cdot log(\\pi_i) - y_i \\cdot log(1 - \\pi_i) + log(1 - \\pi_i) \\right] \\\\ &amp;= \\sum_{i = 1}^{n} \\left[ y_i \\cdot log(\\frac{\\pi_i}{1 - \\pi_i}) + log(1 - \\pi_i) \\right] \\end{align} \\] We can derive the following: \\[ \\begin{align} 1 - \\pi &amp;= 1 - \\frac{exp(\\beta_0 + \\beta_1 x_i)}{1 + exp(\\beta_0 + \\beta_1 x_i)} \\\\ &amp;= \\frac{1 + exp(\\beta_0 + \\beta_1 x_i)}{1 + exp(\\beta_0 + \\beta_1 x_i)} - \\frac{exp(\\beta_0 + \\beta_1 x_i)}{1 + exp(\\beta_0 + \\beta_1 x_i)} \\\\ &amp;= \\frac{1}{1 + exp(\\beta_0 + \\beta_1 x_i)} \\\\ &amp;= (1 + exp(\\beta_0 + \\beta_1 x_i))^{-1} \\end{align} \\] This means the last expression for the log-likelihood can be simplified: \\[ \\begin{align} l(\\beta) = log(L(\\beta)) &amp;= \\sum_{i = 1}^{n} \\left[ y_i \\cdot log(\\frac{\\pi_i}{1 - \\pi_i}) + log(1 - \\pi_i) \\right] \\\\ &amp;= \\sum_{i = 1}^{n} \\bigg[ y_i \\cdot (\\beta_0 + \\beta_1 x_i) - log(1 + exp(\\beta_0 + \\beta_1 x_i)) \\bigg] \\end{align} \\] The maximum likelihood estimates are those values for the model parameters that minimize the log-likelihood. We can manually look for these maximum likelihood estimates. 3.2 Conditional Logistic Regression In this chapter, we explore conditional logistic regression. Conditional logistic regression allows us to not have to estimate nuisance parameters. We will illustrate this with an example. Suppose we have a dataset comprising data on 20 patients distributed across two hospitals (with 10 patients each). For each patient, we know what hospital they were admitted to, their age at the time of hospital admission, and if they survived their hospital stay. # Create the dataset set.seed(345) df &lt;- tibble::tibble( i = 1L:20L, hospital = factor(x = rep(c(&quot;A&quot;, &quot;B&quot;), each = 10), levels = c(&quot;A&quot;, &quot;B&quot;)), age = c(65, 70, 75, 60, 68, 72, 80, 78, 85, 76, 55, 60, 65, 70, 75, 80, 85, 90, 72, 78), Y = c(1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1) ) ## # A tibble: 20 × 4 ## i hospital age Y ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 A 65 1 ## 2 2 A 70 0 ## 3 3 A 75 1 ## 4 4 A 60 0 ## 5 5 A 68 1 ## 6 6 A 72 0 ## 7 7 A 80 0 ## 8 8 A 78 0 ## 9 9 A 85 0 ## 10 10 A 76 0 ## 11 11 B 55 1 ## 12 12 B 60 1 ## 13 13 B 65 1 ## 14 14 B 70 0 ## 15 15 B 75 1 ## 16 16 B 80 1 ## 17 17 B 85 0 ## 18 18 B 90 1 ## 19 19 B 72 0 ## 20 20 B 78 1 3.2.1 Definition Let \\(Y_i\\) denote the outcome for patient \\(i\\), with \\(Y_i = 1\\) if patient \\(i\\) died and \\(Y_i = 0\\) otherwise. Let \\(X_i\\) denote the age of patient \\(i\\) at the time of hospital admission. Let \\(Z_i\\) denote the hospital at which patient \\(i\\) was admitted, with \\(Z_i = 0\\) if patient \\(i\\) was admitted to hospital A and \\(Z_i = 1\\) otherwise. We can now calculate the probability of patient \\(i\\) dying conditional on their age and the hospital where they were admitted to as: \\[ \\mathbb{P} (Y_i = 1) = \\frac{exp(\\beta_z \\cdot z_i + \\beta_x \\cdot x_i)}{1 + exp(\\beta_z \\cdot z_i + \\beta_x \\cdot x_i)} \\] We know that \\(\\beta_z = 0.24\\) and \\(\\beta_x = 0.5\\). We can now calculate the probability of dying for each patient. # Define the parameters B_z &lt;- 1.85 B_x &lt;- 0.05 # Calculate the probability of dying per patient df &lt;- df %&gt;% dplyr::rowwise() %&gt;% dplyr::mutate(Z_i = dplyr::if_else(hospital == &quot;A&quot;, 0, 1), X_i = age) %&gt;% dplyr::mutate(P_i = exp(Z_i * B_z + X_i * B_x) / (1 + exp(Z_i * B_z + X_i * B_x))) ## # A tibble: 20 × 7 ## # Rowwise: ## i hospital age Y Z_i X_i P_i ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 A 65 1 0 65 0.963 ## 2 2 A 70 0 0 70 0.971 ## 3 3 A 75 1 0 75 0.977 ## 4 4 A 60 0 0 60 0.953 ## 5 5 A 68 1 0 68 0.968 ## 6 6 A 72 0 0 72 0.973 ## 7 7 A 80 0 0 80 0.982 ## 8 8 A 78 0 0 78 0.980 ## 9 9 A 85 0 0 85 0.986 ## 10 10 A 76 0 0 76 0.978 ## 11 11 B 55 1 1 55 0.990 ## 12 12 B 60 1 1 60 0.992 ## 13 13 B 65 1 1 65 0.994 ## 14 14 B 70 0 1 70 0.995 ## 15 15 B 75 1 1 75 0.996 ## 16 16 B 80 1 1 80 0.997 ## 17 17 B 85 0 1 85 0.998 ## 18 18 B 90 1 1 90 0.998 ## 19 19 B 72 0 1 72 0.996 ## 20 20 B 78 1 1 78 0.997 3.2.2 Probability In this section, we outline how to get the probability of a particular outcome occurring. Suppose we have an ICU with five patients. For each patient, the presence of diabetes as an underlying comorbidity is recorded. Let \\(X = 1\\) denote that the patient has diabetes and \\(X = 0\\) otherwise. The probability of a patient having diabetes depends on several factors, such as age, sex, weight, lifestyle, genetics, etc, and therefore this probability differs between patients. # Construct a sample dataset probs &lt;- tibble::tibble(i = 1L:5L, P_i = runif(n = 5)) In the following table, we see that the probability that patient \\(i = 1\\) has diabetes is \\(\\mathbb{P} (X_1 = 1) = 0.312\\) and for patient \\(i = 4\\) it is \\(\\mathbb{P} (X_4 = 1) = 0.887\\). ## # A tibble: 5 × 2 ## i P_i ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.312 ## 2 2 0.582 ## 3 3 0.638 ## 4 4 0.887 ## 5 5 0.248 We can also calculate the probability of a patient not having diabetes. For this, the following probability mass function can be used: \\[ f(Y_i = y_1, \\pi_i) = \\pi_i^{y_i} \\cdot (1 - \\pi_i)^{(1 - y_i)} \\] We can verify this: \\[ f(Y_4 = 1, \\pi_4) = 0.887^1 * (1 - 0.887)^{(1 - 1)} = 0.887 \\] and \\[ f(Y_4 = 0, \\pi_4) = 0.887^0 * (1 - 0.887)^{(1 - 0)} = 0.113 \\] Let the random variable \\(Y\\) denote the number of patients in the ICU having diabetes. First, we must construct the sample space. One example of a possible outcome in the sample space is: \\[ (X_1 = 1, X_2 = 0, X_3 = 0, X_4 = 0, X_5 = 1) \\] This possible outcome maps to \\(Y = 2\\). Another example of a possible outcome: \\[ (X_1 = 0, X_2 = 1, X_3 = 1, X_4 = 0, X_5 = 1) \\] This possible outcome maps to \\(Y = 3\\). Another example of a possible outcome: \\[ (X_1 = 0, X_2 = 0, X_3 = 1, X_4 = 1, X_5 = 0) \\] This possible outcome maps to \\(Y = 2\\). Note that we now have two possible outcomes in the sample space that map to \\(Y = 2\\). The sample space contains 32 possible outcomes: \\[ {5 \\choose 0} + {5 \\choose 1} + {5 \\choose 2} + {5 \\choose 3} + {5 \\choose 4} + {5 \\choose 5} = 32 \\] The following are five examples of the sample space: ## # A tibble: 5 × 5 ## Patient_1 Patient_2 Patient_3 Patient_4 Patient_5 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 0 ## 2 0 0 0 0 1 ## 3 0 0 0 1 0 ## 4 0 0 0 1 1 ## 5 0 0 1 0 0 We can now calculate the probability of each possible outcome occurring. For the possible outcome \\[ (X_1 = 0, X_2 = 1, X_3 = 1, X_4 = 0, X_5 = 1) \\] we have \\[ \\begin{aligned} &amp; \\mathbb{P} (Y_1 = 0) \\cdot \\mathbb{P} (Y_2 = 1) \\cdot \\mathbb{P} (Y_3 = 1) \\cdot \\mathbb{P} (Y_4 = 0) \\cdot \\mathbb{P} (Y_5 = 1) \\\\ &amp;= \\prod_{i = 1}^{5} \\bigg[ f(Y_i = y_i, \\pi_i) \\bigg] \\\\ &amp;= f(Y_1 = 0, \\pi_1) \\cdot f(Y_2 = 1, \\pi_2) \\cdot f(Y_3 = 1, \\pi_3) \\cdot f(Y_4 = 0, \\pi_4) \\cdot f(Y_5 = 1, \\pi_5) \\\\ &amp;= (1 - 0.3121850) \\cdot 0.5821874 \\cdot 0.6381059 \\cdot (1 - 0.8872664) \\cdot 0.2481097 \\\\ &amp;= 0.007147012 \\end{aligned} \\] We can calculate this as follows: # Get the observed outcome Y_vector &lt;- c(0, 1, 1, 0, 1) # Calculate the probability prob_Y &lt;- prod(probs$P_i^Y_vector * (1 - probs$P_i)^(1 - Y_vector)) prob_Y ## [1] 0.007147012 This is the probability of this possible outcome occurring. This possible outcome maps to \\(Y = 3\\). If we want to know \\(\\mathbb{P} (Y = 3)\\), we must sum the probabilities of every possible outcome that maps to \\(Y = 3\\). We define the set of all possible outcomes mapping to \\(Y = 3\\) as \\(S(t)\\) where \\(t = 3\\): \\[ S(t) = \\{ (Y_1 = y_1^*, ..., Y_5 = y_5^*) | \\sum_{i = 1}^{5} y_i^* = t \\} \\] In the case of \\(Y = 3\\), \\(S(3)\\) contains ten possible outcomes. We calculate the probability of \\(Y = 3\\) as follows: \\[ \\mathbb{P} (Y = 3) = \\sum_{S(3)} \\Bigg[ \\prod_{i = 1}^{5} \\bigg[ f(Y_i = y_i^*, \\pi_i) \\bigg] \\Bigg] \\] We can calculate this as follows: # Get the subset of the sample space that maps to Y = 3 event_space &lt;- sample_space[rowSums(sample_space) == 3,, drop = F] # Calculate the probability prob_Y &lt;- 0.0 for (i in 1:nrow(event_space)) { # Get the observed outcome Y_vector &lt;- as.integer(event_space[i,, drop = TRUE]) prob_Y &lt;- prob_Y + prod((probs$P_i)^Y_vector * (1 - probs$P_i)^(1 - Y_vector)) } ## [1] 0.3736777 So, we have \\(\\mathbb{P} (Y = 3) = 0.3736777\\). As stated earlier, the probability of a patient \\(i\\) having diabetes depends on several factors, such as sex, age, weight, etc. The probability of a patient \\(i\\) having diabetes is denoted as \\(\\pi_i = \\mathbb{P} (Y_i = 1)\\). This probability can be derived as follows: \\[ \\pi_i = \\frac{exp(\\alpha + \\beta \\textbf{X}^{-1})}{1 + exp(\\alpha + \\beta \\textbf{X}^{-1})} \\] We can use this to rewrite the following: \\[ \\begin{align} f(Y_i = y_i, \\pi_i) &amp;= \\pi_i^{y_i} * (1 - \\pi_i)^{(1 - y_i)} \\\\ &amp;= \\frac{exp(y_i \\cdot (\\alpha + \\beta \\textbf{X}^{-1}))}{1 + exp(\\alpha + \\beta \\textbf{X}^{-1})} \\end{align} \\] For a particular possible outcome of the sample space, we have: \\[ \\begin{align} \\prod_{i = 1}^5 f(Y_i = y_i, \\pi_i) &amp;= \\prod_{i = 1}^5 \\frac{exp(y_i \\cdot (\\alpha + \\beta \\textbf{X}^{-1}))}{1 + exp(\\alpha + \\beta \\textbf{X}^{-1})} \\\\ &amp;= \\frac{\\prod_{i = 1}^5 exp(y_i \\cdot (\\alpha + \\beta \\textbf{X}^{-1}))}{\\prod_{i = 1}^5 \\big[ 1 + exp(\\alpha + \\beta \\textbf{X}^{-1}) \\big]} \\\\ &amp;= \\frac{\\prod_{i = 1}^5 exp(y_i \\alpha + y_i \\beta \\textbf{X}^{-1}))}{\\prod_{i = 1}^5 \\big[ 1 + exp(\\alpha + \\beta \\textbf{X}^{-1}) \\big]} \\\\ &amp;= \\frac{exp(\\sum_{i = 1}^5 \\big[ y_i \\alpha + y_i \\beta \\textbf{X}^{-1} \\big] )}{\\prod_{i = 1}^5 \\big[ 1 + exp(\\alpha + \\beta \\textbf{X}^{-1}) \\big]} \\\\ &amp;= \\frac{exp(\\sum_{i = 1}^5 y_i \\alpha + \\sum_{i = 1}^5 y_i \\beta \\textbf{X}^{-1})}{\\prod_{i = 1}^5 \\big[ 1 + exp(\\alpha + \\beta \\textbf{X}^{-1}) \\big]} \\\\ &amp;= \\frac{exp(\\alpha \\sum_{i = 1}^5 y_i + \\sum_{i = 1}^5 y_i \\beta \\textbf{X}^{-1})}{\\prod_{i = 1}^5 \\big[ 1 + exp(\\alpha + \\beta \\textbf{X}^{-1}) \\big]} \\\\ \\end{align} \\] 3.2.3 Conditional Probability We can calculate the probability \\(\\mathbb{P} (Y = y)\\). But we can also calculate the conditional probability of a particular possible outcome given the event of interest. Suppose we are not interested in the following: \\[ \\mathbb{P} (X_1 = 0, X_2 = 1, X_3 = 1, X_4 = 0, X_5 = 1) \\] but in the following: \\[ \\mathbb{P} (X_1 = 0, X_2 = 1, X_3 = 1, X_4 = 0, X_5 = 1 | Y = 3) \\] We can calculate this as follows: \\[ \\begin{align} \\mathbb{P} (X_1 = 0, X_2 = 1, X_3 = 1, X_4 = 0, X_5 = 1 | Y = 3) &amp;= \\frac{\\mathbb{P} (X_1 = 0, X_2 = 1, X_3 = 1, X_4 = 0, X_5 = 1)}{\\mathbb{P} (Y = 3)} \\\\ &amp;= \\frac{\\mathbb{P} (X_1 = 0, X_2 = 1, X_3 = 1, X_4 = 0, X_5 = 1)}{\\sum_{S(3)} \\Bigg[ \\prod_{i = 1}^{5} \\bigg[ f(Y_i = y_i^*, \\pi_i) \\bigg] \\Bigg]} \\end{align} \\] 3.2.4 Probability Before continuing our explanation on conditional logistic regression, we focus on how to calculate the probability of a particular outcome occurring. For this, we first focus on a single hospital, i.e., we focus on hospital A. ## # A tibble: 10 × 7 ## # Rowwise: ## i hospital age Y Z_i X_i P_i ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 A 65 1 0 65 0.963 ## 2 2 A 70 0 0 70 0.971 ## 3 3 A 75 1 0 75 0.977 ## 4 4 A 60 0 0 60 0.953 ## 5 5 A 68 1 0 68 0.968 ## 6 6 A 72 0 0 72 0.973 ## 7 7 A 80 0 0 80 0.982 ## 8 8 A 78 0 0 78 0.980 ## 9 9 A 85 0 0 85 0.986 ## 10 10 A 76 0 0 76 0.978 Hospital A has 10 patients, and there are two possible outcomes for each patient, dead or alive. The outcome of interest \\(Y\\) is the number of patients that have died during their hospital stay. The data frame above shows one possible outcome for hospital A where two out of ten patients died during their hospital stay. More specifically, patients \\(i = 1\\) and \\(i = 5\\) died. This is only one of a few possible outcomes where two patients die. There are, in fact, \\(10 \\choose 2 = 45\\) possible outcomes in which two patients die. The total number of possible outcomes is shown below, where \\(Y\\) denotes how many patients died during their hospital stay and \\(SS\\) denotes how many of the possible outcomes in the sample space map to \\(Y\\). tibble::tibble( Y = 0L:10L, SS = choose(10, Y) ) ## # A tibble: 11 × 2 ## Y SS ## &lt;int&gt; &lt;dbl&gt; ## 1 0 1 ## 2 1 10 ## 3 2 45 ## 4 3 120 ## 5 4 210 ## 6 5 252 ## 7 6 210 ## 8 7 120 ## 9 8 45 ## 10 9 10 ## 11 10 1 So, with \\(Y = 9\\), there are 9 possible outcomes mapping to this outcome. We can define the set of possible outcomes mapping to \\(Y = 9\\) as follows: \\[ S(t) = \\{ (Y_1 = y_1^*, ..., Y_{10} = y_{10}^*) | \\sum_{i = 1}^{10} y_i^* = t \\} \\] with \\(t = y\\). In this case, \\(S(t)\\) denotes the set of possible outcomes mapping to \\(Y = t\\). If we know the probability of each patient \\(i\\) dying, then, for each possible outcome we can calculate the probability of that outcome occurring as follows: \\[ \\begin{align} \\mathbb{P} (\\textbf{Y} = \\textbf{y}) = \\mathbb{P} (Y_1 = y_1, ..., Y_{10} = y_{10}) &amp;= \\mathbb{P} (Y_1 = y_1) \\times \\text{ ... } \\times\\mathbb{P} (Y_{10} = y_{10}) \\end{align} \\] If we want to know the probability of \\(Y = 3\\), then we must first obtain the probabilities for each possible outcome in \\(S(3)\\) and sum these up. So we have: \\[ \\sum_{S(t)} \\mathbb{P} (\\textbf{Y} = \\textbf{y} | t = 3) \\] We can calculate this: # Construct the sample space sample_space &lt;- gtools::permutations( n = 2, r = 10, v = c(0, 1), repeats.allowed = TRUE ) sample_space &lt;- tibble::as_tibble(sample_space, .name_repair = &quot;minimal&quot;) # Get the subset of possible outcomes corresponding to Y = 3 event_space &lt;- sample_space[rowSums(sample_space) == 3,, drop = F] # Calculate the probability of Y = 3 prob_Y &lt;- 0.0 probs &lt;- df %&gt;% dplyr::filter(hospital == &quot;A&quot;) %&gt;% dplyr::pull(P_i) for (i in 1L:nrow(event_space)) { # Get the outcomes for each patient corresponding to Y = 3 Y &lt;- as.integer(event_space[i,, drop = TRUE]) # Calculate the probability of this particular outcome occurring prob_Y &lt;- prob_Y + prod(probs^Y * (1 - probs)^(1 - Y)) } prob_Y ## [1] 8.732308e-10 \\[ \\mathbb{P} (\\textbf{Y} = \\textbf{y}) = \\mathbb{P} (Y_1 = y_1, Y_2 = y_2, ..., Y_n = Y_n) &amp;= \\frac{exp(\\beta_z \\cdot z_i + \\beta_x \\cdot x_i)}{1 + exp(\\beta_z \\cdot z_i + \\beta_x \\cdot x_i)} \\] Now, suppose we are only interested in \\(\\beta_x\\) and not in \\(\\beta_z\\), the latter being a nuisance parameter. We would like to eliminate \\(\\beta_z\\) from the estimation. We can do this by conditioning the likelihood function. This results in an estimate only for age \\(X_i\\). Within each hospital, we want to calculate the probability of obtaining a particular outcome, e.g., 3 out of 10 patients died. Let us look at our two hospitals and the outcomes. We can then see that 3 out of 10 patients died in hospital A and 7 out of 10 patients in hospital B. table(df$hospital, df$Y) ## ## 0 1 ## A 7 3 ## B 3 7 We can calculate the probability of this outcome for each hospital. To calculate the probability of a single patient dying, we have: \\[ \\mathbb{P} (Y_i = 1) = \\frac{exp(\\beta_z z_i + \\beta_x x_i)}{1 + exp(\\beta_z z_i + \\beta_x x_i)} \\] Or if we do not know what the outcome is but we want to calculate the probability of that outcome: \\[ \\mathbb{P} (Y_i = y_i) = \\frac{exp(y_i \\cdot (\\beta_z z_i + \\beta_x x_i))}{1 + exp(\\beta_z z_i + \\beta_x x_i)} \\] We have ten patients per hospital. If we want to calculate the probability of a particular outcome occurring, we have: \\[ \\begin{align} \\mathbb{P} (Y_1 = y_1, ..., Y_n = y_n) &amp;= \\prod_{i = 1}^{n} \\frac{exp(y_i \\cdot (\\beta_z z_i + \\beta_x x_i))}{1 + exp(\\beta_z z_i + \\beta_x x_i)} \\\\ &amp;= \\frac{\\prod_{i = 1}^{n} exp(y_i \\cdot (\\beta_z z_i + \\beta_x x_i))}{\\prod_{i = 1}^{n} [1 + exp(\\beta_z z_i + \\beta_x x_i)]} \\\\ &amp;= \\frac{\\prod_{i = 1}^{n} exp(y_i \\beta_z z_i + y_i \\beta_x x_i))}{\\prod_{i = 1}^{n} [1 + exp(\\beta_z z_i + \\beta_x x_i)]} \\\\ &amp;= \\frac{exp(\\sum_{i = 1}^{n} y_i \\beta_z z_i + \\sum_{i = 1}^{n} y_i \\beta_x x_i))}{\\prod_{i = 1}^{n} [1 + exp(\\beta_z z_i + \\beta_x x_i)]} \\\\ &amp;= \\frac{exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i))}{\\prod_{i = 1}^{n} [1 + exp(\\beta_z z_i + \\beta_x x_i)]} \\end{align} \\] We can now calculate the probability of the outcome \\(Y_1 = 1\\), \\(Y_3 = 1\\), and \\(Y_8 = 1\\) in hospital A occurring. Note that we specify three particular patients that have died. But if we are only interested in 3 out of 10 patients dying in hospital A. In that case, there are \\({10 \\choose 3} = 120\\) outcomes of interest. First, we define this set of 120 outcomes: \\[ S(t) = \\{ (Y_1 = y_1, Y_2, = y_2, ..., Y_n = y_n) | \\sum_{i = 1}^{n} y_i = t \\} \\] To calculate the probability of \\(S(t)\\) occurring, we can calculate this as follows: \\[ \\begin{align} \\mathbb{P} (S(t)) &amp;= \\sum_{S(t)} \\mathbb{P} (Y_1 = y_i, ..., Y_n = y_n) \\\\ &amp;= \\sum_{S(t)} [\\frac{exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i))}{\\prod_{i = 1}^{n} [1 + exp(\\beta_z z_i + \\beta_x x_i)]}] \\\\ &amp;= \\frac{1}{\\prod_{i = 1}^{n} [1 + exp(\\beta_z z_i + \\beta_x x_i)]} \\cdot \\sum_{S(t)} [exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i))] \\end{align} \\] We can now calculate the conditional probability of the outcome \\(Y_1 = 1\\), \\(Y_3 = 1\\), and \\(Y_8\\) conditional on that 3 out of 10 patients died in hospital A. \\[ \\begin{align} \\mathbb{P} (Y_1 = 0, Y_2 = 0, Y_3 = 1, ..., Y_{10} = 0 | \\sum_{i} y_i = 10) &amp;= \\frac{\\mathbb{P} (Y_1 = y_1, ..., Y_{10} = y_{10})}{\\mathbb{P} (S(t = 3))} \\\\ &amp;= \\frac{\\frac{exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i)}{\\prod_{i = 1}^{n} [1 + exp(\\beta_z z_i + \\beta_x x_i)]}}{\\frac{1}{\\prod_{i = 1}^{n} [1 + exp(\\beta_z z_i + \\beta_x x_i)]} \\cdot \\sum_{S(t)} [exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i))]} \\\\ &amp;= \\frac{exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i)}{\\sum_{S(t)} [exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i)]} \\end{align} \\] Because we look at each hospital individually, we know what the value is for \\(z_i\\) for each patient. If we look at hospital B, we know that \\(z_i = 1\\) for \\(i = 1, ..., 10\\). In that case, we have: \\[ \\begin{align} \\mathbb{P} (Y_1 = 0, Y_2 = 0, Y_3 = 1, ..., Y_{10} = 0 | \\sum_{i} y_i = 10) &amp;= \\frac{exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i)}{\\sum_{S(t)} [exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i)]} \\\\ &amp;= \\frac{exp(\\beta_z \\sum_{i = 1}^{n} y_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i)}{\\sum_{S(t)} [exp(\\beta_z \\sum_{i = 1}^{n} y_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i)]} \\\\ &amp;= \\frac{exp(\\beta_z t + \\beta_x \\sum_{i = 1}^{n} y_i x_i)}{\\sum_{S(t)} [exp(\\beta_z t + \\beta_x \\sum_{i = 1}^{n} y_i x_i)]} \\\\ &amp;= \\frac{exp(\\beta_z t) exp(\\beta_x \\sum_{i = 1}^{n} y_i x_i)}{\\sum_{S(t)} [exp(\\beta_z t) exp(\\beta_x \\sum_{i = 1}^{n} y_i x_i)]} \\\\ &amp;= \\frac{exp(\\beta_z t) exp(\\beta_x \\sum_{i = 1}^{n} y_i x_i)}{exp(\\beta_z t) \\sum_{S(t)} [exp(\\beta_x \\sum_{i = 1}^{n} y_i x_i)]} \\\\ &amp;= \\frac{exp(\\beta_x \\sum_{i = 1}^{n} y_i x_i)}{\\sum_{S(t)} [exp(\\beta_x \\sum_{i = 1}^{n} y_i x_i)]} \\end{align} \\] This is the conditional probability. We can use this to determine the conditional likelihood and log-likelihood, in which we do not have to estimate the nuisance parameter \\(\\beta_z\\). "],["missingness.html", "4 Missingness 4.1 Introduction 4.2 Missingess Patterns", " 4 Missingness 4.1 Introduction When fitting models to a particular data set, missingness is of particular concern. Missing data can lead to biased coefficient estimates. Therefore, a good understanding of the different missingness patterns, their implications on model fitting, and how to deal with them is essential. There are three missingness patterns (missingness mechanisms): missingness completely at random (MCAR) missingness at random (MAR) missingness not at random (MNAR). In the following sections, we will use a simulated data set containing data on patients and their weight and BMI to illustrate these different mechanisms. 4.2 Missingess Patterns 4.2.1 Simulation Setup Our simulation setup involves simulating the weight of patients visiting their general physician (GP) for a checkup. For each patient, the sex (male, female) is always recorded. The data set is balanced in terms of sex, and we assume a normally distributed weight (kg) with a mean weight \\(\\mu_{f} = 65\\) for women and \\(\\mu_{m} = 78\\) for men. The standard deviation \\(\\sigma = 10\\) is the same for both sexes: \\[ W_{f} \\sim \\mathcal{N}(65, 100) \\] and \\[ W_{m} \\sim \\mathcal{N}(78, 100). \\] We also simulate the BMI for these patients. We assume the BMI is normally distributed as follows: \\[ BMI \\sim \\mathcal{N}(25, 9). \\] # Simulate patient data. The data set contains 10000 observations, and is # balanced in terms of sex. set.seed(123) n &lt;- 10000 # Define the parameters that define the distribution of weight and BMI sd_bmi &lt;- 3 mu_bmi &lt;- 25 sd_weight &lt;- 10 mu_weight_f &lt;- 65 mu_weight_m &lt;- 78 weights &lt;- tibble::tibble( idx = 1:n, sex = rep(x = c(&quot;F&quot;, &quot;M&quot;), each = n / 2), weight = c( rnorm(n / 2, mu_weight_f, sd_weight), rnorm(n / 2, mu_weight_m, sd_weight) ), bmi = round(rnorm(n, mu_bmi, sd_bmi), 1) ) We can see that the conditional distributions of weight given sex have different means. But the conditional distributions of BMI are the same. 4.2.2 Missingness Completely at Random Missingness completely at random indicates that the observations with missing values are a random subset of all observations. There are no factors that affect the probability of missingness, nor is the probability of missingess affected by the outcome itself (weight or BMI). The distribution of observed and missing values is the same. # Simulate MCAR: assume that 30% of observations have a missing outcome set.seed(123) idxs &lt;- sample(x = n, size = n * 0.3) weights[&quot;mcar&quot;] &lt;- weights$idx %in% idxs Because the observations with missing values are a random subset of all observations, we should see no difference between the distributions of missing and observed values. 4.2.3 Missingness at Random With missingness at random, we believe there are certain factors that affect the probability of missingness, and that these same factors also have an effect on the outcome. In our example, we see that sex affects the weight. We now assume that sex also affects the probability of missingness. # Determine the conditional probabilities of missingness p_missing_f &lt;- 0.45 p_missing_m &lt;- 0.25 idxs &lt;- c( sample( x = weights[weights$sex == &quot;F&quot;, &quot;idx&quot;, drop = T], size = (n / 2) * p_missing_f ), sample( x = weights[weights$sex == &quot;M&quot;, &quot;idx&quot;, drop = T], size = (n / 2) * p_missing_m ) ) weights[&quot;mar&quot;] &lt;- weights$idx %in% idxs Inspection of the distributions of observed and missing values shows us that the distribution of missing values is shifted to the left compared to the distribution of observed values. Because the probability of missingness is higher for women then for men, the fraction of patients with a missing weight that are female is higher than the fraction of patients with a missing weight that are men. This affects the distribution of the missing weights, i.e., shifting the distribution to the left. Assume that sex is the only factor that affects the probability of missingness, then, conditional on sex, the distribution of observed values and missing values is the same. We can see that, conditional on sex, the distribution of observed and missing weights is the same. This illustrates the concept of missingness at random. Sex is the factor that influences both the probability of missingness and the outcome itself. Conditioning on this factors removes the difference in distributions of observed and missing weights. In practice, it is impossible to conclude missingness at random by comparing the distribution of observed and missing values as the missing values are, of course, missing. This example just demonstrates the concept of missingness at random. 4.2.4 Missingness Not at Random With missingness not at random, the probability of missingness is not affected by a factor that is recorded, but is affected by factors that are not recorded or even by the outcome itself. For this missingness pattern, we will not focus on the weight of the patient but on the patient’s BMI. The distribution of BMI is the same for women and for men: However, we could argue the following: if a patient visits the GP, then the GP will more likely weigh the patient if he/she seems overweight or obese. We could make the same argument for underweight patients, but for the sake of illustration, we will assume the likelihood of being weighted is higher for overweight and obese patients. Based on BMI, we can classify patients as: underweight: \\(BMI &lt; 18.5\\) normal: \\(18.5 \\leq BMI \\leq 24.9\\) overweight: \\(25 \\leq BMI \\leq 29.9\\) obese: \\(30 \\leq BMI\\). We assume the following conditional probabilities of missingness: # Define conditional probabilities of missingness p_missing_uw_nw &lt;- 0.75 # underweight and normal weight p_missing_ow_ob &lt;- 0.55 # overweight and obese # Classify patients based on their BMI weights &lt;- weights %&gt;% dplyr::mutate( bmi_class = dplyr::case_when( bmi &lt; 18.5 ~ &quot;uw&quot;, dplyr::between(bmi, 18.5, 24.9) ~ &quot;nw&quot;, dplyr::between(bmi, 25.0, 29.9) ~ &quot;ow&quot;, bmi &gt;= 30.0 ~ &quot;ob&quot; ) ) fd_bmi_classes_df &lt;- weights %&gt;% dplyr::group_by(bmi_class) %&gt;% dplyr::summarize(n = n()) fd_bmi_classes &lt;- fd_bmi_classes_df$n names(fd_bmi_classes) &lt;- fd_bmi_classes_df$bmi_class set.seed(456) idxs &lt;- c( sample( x = weights[weights$bmi_class %in% c(&quot;uw&quot;, &quot;nw&quot;), &quot;idx&quot;, drop = T], size = round((fd_bmi_classes[&quot;uw&quot;] + fd_bmi_classes[&quot;nw&quot;]) * p_missing_uw_nw) ), sample( x = weights[weights$bmi_class %in% c(&quot;ow&quot;, &quot;ob&quot;), &quot;idx&quot;, drop = T], size = round((fd_bmi_classes[&quot;ow&quot;] + fd_bmi_classes[&quot;ob&quot;]) * p_missing_ow_ob) ) ) weights[&quot;mnar&quot;] &lt;- weights$idx %in% idxs First, let’s have a look at the distribution of BMI conditional on missingness. The distribution of the missing BMI values is shifted to the left compared to the distribution of the observed BMI values. This makes sense as the probability of the weight being recorded, and thus the BMI being calculated, is bigger as a patient tends to be overweight or obese. We know from our simulation that sex does not influence the probability of missigness, nor is there any other factor that influences the probability of missingness, only the outcome itself. Therefore, conditioning on sex, will not eliminate the difference in distribution between observed and missing values. "],["relationships-and-regression.html", "5 Relationships and Regression 5.1 Relationships 5.2 Pearson Correlation Coefficient 5.3 Spearman Rank Correlation", " 5 Relationships and Regression 5.1 Relationships Two random variables are related if changes in one variable are consistently associated with changes in another variable. Or, in other words, when the two random variables tend to change together. We say that the two variables have a relationship (or relation). When describing relationships between variables, we must take into account (1) the pattern, (2) the direction, and (3) the strength of the relationship. The pattern of the relationship focuses on the shape of the regression curve of the relationship. This can be linear, quadratic, exponential, etc. The relationship can be positive or negative (direction). And the relationship can be strong or weak. We will come back on the topic of strength of a relationship when discussing the Pearson correlation coefficient. As an example, let us have a look at the Orange dataset. This dataset contains 35 observations on two random variables: age in days and circumference in millimeters. In the following figure, we see that an increase in age tends to be associated with an increase in circumference. Or, in other words, relatively low values for age tend to occur with relatively low values for circumference, and relatively high values for age tend to occur with relatively high values for circumference. This indicates a positive relationship. When drawing a smoother through the data, it seems the relationship can be described with an approximately straight line, making it a linear relationship. ggplot(Orange, aes(x = age, y = circumference)) + geom_point() + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;, se = FALSE) + labs(x = &quot;Age (days)&quot;, y = &quot;Circumference (mm)&quot;) + theme_classic() In the following sections, we will focus on statistical measures that can be used to describe linear relationships. 5.2 Pearson Correlation Coefficient The Pearson correlation coefficient is a statistical measure that quantifies the direction and strength of a linear relationship between two variables. The sign of the Pearson correlation coefficient indicates the direction and the absolute value, ranging from \\(0\\) to \\(1\\), indicates the strength of the relationship. 5.2.1 Calculating Coefficient The Pearson correlation coefficient is calculated as follows: \\[ \\rho = \\frac{Cov(X, Y)}{\\sigma_{X} \\sigma_{Y}} = \\frac{\\sigma_{XY}}{\\sigma_{X} \\sigma_{Y}} \\] Note that \\(\\rho\\) denotes the population Pearson correlation coefficient, while \\(r\\) denotes the sample correlation coefficient. The latter is calculated as follows: \\[ r = \\frac{s_{XY}}{s_{X} s_{Y}} \\] Dividing the covariance between \\(X\\) and \\(Y\\) by the product of the standard deviations of \\(X\\) and \\(Y\\) ensures that the correlation coefficient is not dependent on the scale of the variables \\(X\\) and \\(Y\\) and that \\(\\rho \\in [-1, +1]\\). If \\(\\rho = +1\\), then all of the data falls on a single straight line with a positive slope. Some examples: # Create a sample dataset dat &lt;- tibble(X = runif(n = 20, min = 10, max = 80)) dat &lt;- add_column(dat, Y1 = 0.30*dat$X) dat &lt;- add_column(dat, Y2 = 3.00*dat$X) dat &lt;- add_column(dat, Y3 = 30.0*dat$X) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y1), formula = &quot;y ~ x&quot;, color = &quot;darkred&quot;, se = F) + geom_point(aes(y = Y2), color = &quot;darkgreen&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y2), formula = &quot;y ~ x&quot;, color = &quot;darkgreen&quot;, se = F) + geom_point(aes(y = Y3), color = &quot;darkblue&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y3), formula = &quot;y ~ x&quot;, color = &quot;darkblue&quot;, se = F) + labs(x = &quot;X&quot;, y = &quot;Y&quot;) + theme_classic() Note that these examples illustrate a functional relationship. The relationship between \\(X\\) and each of the \\(Y\\) variables (\\(Y1\\), \\(Y2\\), \\(Y3\\)) can be described by an equation and all the observations fall on the line of the relationship. If we compute the Pearson correlation coefficients for each of these three pairs of variables, we see that it is each time equal to \\(r = +1\\). cor(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## [1] 1 cor(x = dat$X, y = dat$Y2, method = &quot;pearson&quot;) ## [1] 1 cor(x = dat$X, y = dat$Y3, method = &quot;pearson&quot;) ## [1] 1 If we now use a negative slope, we will see that the Pearson correlation coefficient for each of these functional relationships equals \\(r = -1\\). # Create a sample dataset dat &lt;- tibble(X = runif(n = 20, min = 10, max = 80)) dat &lt;- add_column(dat, Y1 = -0.30*dat$X) dat &lt;- add_column(dat, Y2 = -3.00*dat$X) dat &lt;- add_column(dat, Y3 = -30.0*dat$X) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y1), formula = &quot;y ~ x&quot;, color = &quot;darkred&quot;, se = F) + geom_point(aes(y = Y2), color = &quot;darkgreen&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y2), formula = &quot;y ~ x&quot;, color = &quot;darkgreen&quot;, se = F) + geom_point(aes(y = Y3), color = &quot;darkblue&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y3), formula = &quot;y ~ x&quot;, color = &quot;darkblue&quot;, se = F) + labs(x = &quot;X&quot;, y = &quot;Y&quot;) + theme_classic() cor(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## [1] -1 cor(x = dat$X, y = dat$Y2, method = &quot;pearson&quot;) ## [1] -1 cor(x = dat$X, y = dat$Y3, method = &quot;pearson&quot;) ## [1] -1 We see that the absolute value of the Pearson correlation coefficient equals \\(1\\) for the six examples. This indicates that these are very strong linear relationships. But what does it mean for a relationship between two variables to be strong? The strength of a relationship between two variables is an indication of how much information a given value for one variables gives us about the value of the other variable. If we see that for a given value of \\(X\\) the range of values for \\(Y\\) is relatively large, and this for every value of \\(X\\), then we cannot say that \\(X\\) carries much information about \\(Y\\). If, however, the range of values for \\(Y\\) is relatively small for every value of \\(X\\), then we can state that \\(X\\) carries a lot of information about the value of \\(Y\\) and the relationship is quite strong. We will showcase with a few examples. # Create a sample dataset dat &lt;- tibble(X = runif(100, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 + 5*dat$X + rnorm(100, sd = 1)) dat &lt;- add_column(dat, Y2 = -5 + 5*dat$X + rnorm(100, sd = 10)) dat &lt;- add_column(dat, Y3 = -5 + 5*dat$X + rnorm(100, sd = 25)) For the following three positive linear relationships, we see that the amount of scatter around the regression line is different for the three examples. The amount of scatter around the regression line is an indication of how much information the variable \\(X\\) carries about the variable \\(Y\\). If we look at any given value for \\(X\\), we see that the range of observed values for \\(Y1\\) is relatively small. For \\(Y2\\), the range of observed \\(Y2\\) values is larger. And for \\(Y3\\), the range of observed \\(Y3\\) values is relatively large. Therefore, we can state that the relationship between \\(X\\) and \\(Y1\\) is strong, between \\(X\\) and \\(Y2\\) is moderate, and between \\(X\\) and \\(Y3\\) is weak. # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = 5, color = &quot;darkred&quot;) + labs(x = &quot;X&quot;, y = &quot;Y1&quot;) + scale_y_continuous(limits = c(60, 250)) + theme_classic() # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y2), color = &quot;darkgreen&quot;) + geom_abline(intercept = -5, slope = 5, color = &quot;darkgreen&quot;) + labs(x = &quot;X&quot;, y = &quot;Y2&quot;) + scale_y_continuous(limits = c(60, 250)) + theme_classic() # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y3), color = &quot;darkblue&quot;) + geom_abline(intercept = -5, slope = 5, color = &quot;darkblue&quot;) + labs(x = &quot;X&quot;, y = &quot;Y3&quot;) + scale_y_continuous(limits = c(60, 250)) + theme_classic() The absolute values of the Pearson correlation coefficient are indicative of the strength of the linear relationship. Therefore, we expect the largest Pearson correlation coefficient for \\(X\\) and \\(Y1\\), a smaller one for \\(X\\) and \\(Y2\\), and the smallest one for \\(X\\) and \\(Y3\\). cor(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## [1] 0.9993038 cor(x = dat$X, y = dat$Y2, method = &quot;pearson&quot;) ## [1] 0.9429303 cor(x = dat$X, y = dat$Y3, method = &quot;pearson&quot;) ## [1] 0.703926 5.2.2 Hypothesis Testing All of the above examples on the Pearson correlation coefficient made use of samples of data to calculate the correlation. When analyzing data, we must be be aware that often we are analyzing a sample of data which is drawn from a larger population. For this population, there is a true Pearson correlation coefficient \\(\\rho\\), which is an unknown constant. To estimate \\(\\rho\\), we draw a random sample from the population and use an estimator to produce an estimate \\(\\hat{\\rho}\\). We have already seen the estimator in this chapter. It is: \\[ r = \\frac{s_{XY}}{s_{X} s_{Y}} \\] We must remember that the Pearson correlation coefficient \\(r\\) is an estimator for the true Pearson correlation coefficient \\(\\rho\\) in the population. The latter is an unknown constant. We want to estimate this unknown constant and therefore we sample data from the population and analyze this data to produce an estimate of the true value of \\(\\rho\\). Using this, we can perform a \\(t\\)-test to test for the existence of a linear relationship between two variables. The null hypothesis states that the population correlation coefficient \\(\\rho = 0\\) and that there is no linear relationship: \\[ H_0 : \\rho = 0 \\] The alternative hypothesis states: \\[ H_a : \\rho \\neq 0 \\] The test statistic \\(T\\) for this test is: \\[ T = \\frac{r \\cdot \\sqrt{n - 2}}{\\sqrt{1 - R^2}} \\] The test statistic \\(T\\) follows a \\(t\\)-distribution with \\(n - 2\\) degrees of freedom. cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = 265.15, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9989636 0.9995323 ## sample estimates: ## cor ## 0.9993038 cor.test(x = dat$X, y = dat$Y2, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y2 ## t = 28.032, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9162004 0.9613063 ## sample estimates: ## cor ## 0.9429303 cor.test(x = dat$X, y = dat$Y3, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y3 ## t = 9.811, df = 98, p-value = 3.112e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.5889364 0.7909801 ## sample estimates: ## cor ## 0.703926 In these three examples, we see that the \\(p\\)-values are all statistically significant at \\(\\alpha = 0.05\\). If we look at the test statistic \\(T\\), we see that both sample size \\(n\\) and the estimate \\(r\\) affect the test statistic. We see that larger sample sizes and larger estimates of \\(\\rho\\) result in a larger test statistic. In the following example, we keep the random part of the relationship relatively small. So we are drawing from a population with a strong negative linear relationship. We will use varying sample sizes (from small to large). # Create a sample dataset n &lt;- 3 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 1)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -120.08, df = 1, p-value = 0.005301 ## alternative hypothesis: true correlation is not equal to 0 ## sample estimates: ## cor ## -0.9999653 # Create a sample dataset n &lt;- 30 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 1)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -136.97, df = 28, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9996494 -0.9984158 ## sample estimates: ## cor ## -0.9992546 # Create a sample dataset n &lt;- 50 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 1)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -236.6, df = 48, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9997581 -0.9992411 ## sample estimates: ## cor ## -0.9995715 We will now increae the variability around the regression line of the linear relationship. # Create a sample dataset n &lt;- 3 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 30)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -3.9069, df = 1, p-value = 0.1595 ## alternative hypothesis: true correlation is not equal to 0 ## sample estimates: ## cor ## -0.9687692 # Create a sample dataset n &lt;- 30 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 30)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -4.5246, df = 28, p-value = 0.0001016 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.8185127 -0.3781540 ## sample estimates: ## cor ## -0.6498847 # Create a sample dataset n &lt;- 50 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 30)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -7.2289, df = 48, p-value = 3.281e-09 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.8329269 -0.5551802 ## sample estimates: ## cor ## -0.721961 5.2.3 No Relationship Assume there is no relationship between \\(X\\) and \\(Y\\), then the true Pearson correlation coefficient \\(\\rho = 0\\). Therefore, the null hypothesis should not be rejected. # Create a sample dataset dat &lt;- tibble(X = runif(100, min = 20, max = 40), Y = rnorm(100, mean = -5, sd = 1)) # Visualize the relationships ggplot(dat, aes(x = X, y = Y)) + geom_point(color = &quot;darkred&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;, se = F) + theme_classic() cor.test(x = dat$X, y = dat$Y, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y ## t = -0.51726, df = 98, p-value = 0.6061 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.2460756 0.1457322 ## sample estimates: ## cor ## -0.05217951 Another example. # Create a sample dataset dat &lt;- tibble(X = rnorm(100), Y = rnorm(100)) # Visualize the relationships ggplot(dat, aes(x = X, y = Y)) + geom_point(color = &quot;darkred&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;, se = F) + theme_classic() cor.test(x = dat$X, y = dat$Y, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y ## t = 0.94208, df = 98, p-value = 0.3485 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.1036095 0.2858359 ## sample estimates: ## cor ## 0.09473661 5.2.4 Orange Dataset If we go back to our case study from the trees dataset. ggplot(Orange, aes(x = age, y = circumference)) + geom_point() + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;, se = FALSE) + labs(x = &quot;Age (days)&quot;, y = &quot;Circumference (mm)&quot;) + theme_classic() We already described this relationship. We can now calculate the Pearson correlation coefficient manually. First, we must calculate the covariance. Let the random variable \\(X\\) denote the age and \\(Y\\) the circumference. # Calculate the quantities required to calculate the covariance orange_rel &lt;- as_tibble(Orange) %&gt;% select(X = age, Y = circumference) %&gt;% mutate(X_bar = mean(X), Y_bar = mean(Y)) %&gt;% mutate(X_minus_X_bar = X - X_bar, Y_minus_Y_bar = Y - Y_bar) %&gt;% mutate(cross_product = X_minus_X_bar * Y_minus_Y_bar) kable(orange_rel) X Y X_bar Y_bar X_minus_X_bar Y_minus_Y_bar cross_product 118 30 922.1429 115.8571 -804.14286 -85.8571429 69041.40816 484 58 922.1429 115.8571 -438.14286 -57.8571429 25349.69388 664 87 922.1429 115.8571 -258.14286 -28.8571429 7449.26531 1004 115 922.1429 115.8571 81.85714 -0.8571429 -70.16327 1231 120 922.1429 115.8571 308.85714 4.1428571 1279.55102 1372 142 922.1429 115.8571 449.85714 26.1428571 11760.55102 1582 145 922.1429 115.8571 659.85714 29.1428571 19230.12245 118 33 922.1429 115.8571 -804.14286 -82.8571429 66628.97959 484 69 922.1429 115.8571 -438.14286 -46.8571429 20530.12245 664 111 922.1429 115.8571 -258.14286 -4.8571429 1253.83673 1004 156 922.1429 115.8571 81.85714 40.1428571 3285.97959 1231 172 922.1429 115.8571 308.85714 56.1428571 17340.12245 1372 203 922.1429 115.8571 449.85714 87.1428571 39201.83673 1582 203 922.1429 115.8571 659.85714 87.1428571 57501.83673 118 30 922.1429 115.8571 -804.14286 -85.8571429 69041.40816 484 51 922.1429 115.8571 -438.14286 -64.8571429 28416.69388 664 75 922.1429 115.8571 -258.14286 -40.8571429 10546.97959 1004 108 922.1429 115.8571 81.85714 -7.8571429 -643.16327 1231 115 922.1429 115.8571 308.85714 -0.8571429 -264.73469 1372 139 922.1429 115.8571 449.85714 23.1428571 10410.97959 1582 140 922.1429 115.8571 659.85714 24.1428571 15930.83673 118 32 922.1429 115.8571 -804.14286 -83.8571429 67433.12245 484 62 922.1429 115.8571 -438.14286 -53.8571429 23597.12245 664 112 922.1429 115.8571 -258.14286 -3.8571429 995.69388 1004 167 922.1429 115.8571 81.85714 51.1428571 4186.40816 1231 179 922.1429 115.8571 308.85714 63.1428571 19502.12245 1372 209 922.1429 115.8571 449.85714 93.1428571 41900.97959 1582 214 922.1429 115.8571 659.85714 98.1428571 64760.26531 118 30 922.1429 115.8571 -804.14286 -85.8571429 69041.40816 484 49 922.1429 115.8571 -438.14286 -66.8571429 29292.97959 664 81 922.1429 115.8571 -258.14286 -34.8571429 8998.12245 1004 125 922.1429 115.8571 81.85714 9.1428571 748.40816 1231 142 922.1429 115.8571 308.85714 26.1428571 8074.40816 1372 174 922.1429 115.8571 449.85714 58.1428571 26155.97959 1582 177 922.1429 115.8571 659.85714 61.1428571 40345.55102 We can now calculate the covariance as follows: \\[ Cov(X, Y) = \\frac{\\sum_{i = 1}^{n} (X_i - \\overline{X})(Y_i - \\overline{Y})}{n - 1} \\] Which gives us: sum(orange_rel$cross_product) / (nrow(orange_rel) - 1) ## [1] 25831.02 or: cov(orange_rel$X, orange_rel$Y) ## [1] 25831.02 We can now divide the covariance between \\(X\\) and \\(Y\\) by the product of the sample standard deviations of \\(X\\) and \\(Y\\): (sum(orange_rel$cross_product) / (nrow(orange_rel) - 1)) / (sd(orange_rel$X) * sd(orange_rel$Y)) ## [1] 0.9135189 or: cor(x = orange_rel$X, y = orange_rel$Y, method = &quot;pearson&quot;) ## [1] 0.9135189 Before interpreting the estimate of the Pearson correlation coefficient, we can perform a hypothesis test to check if it differs from 0 at a significance level of \\(\\alpha = 0.05\\). The null and alternative hypotheses state: \\[ H_0 : \\rho = 0 \\text{ vs. } H_a : \\rho \\neq 0 \\] We perform the hypothesis test: cor.test(x = Orange$age, y = Orange$circumference, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: Orange$age and Orange$circumference ## t = 12.9, df = 33, p-value = 1.931e-14 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.8342364 0.9557955 ## sample estimates: ## cor ## 0.9135189 To conclude, we can state that the visualization of the observations and a LOESS curve indicate a positive linear relationship between age and circumference of trees. The Pearson correlation coefficient estimate of \\(r = 0.9135189\\) (\\(p &lt; 0.0001\\)) indicates a very strong positive linear relation between age and circumference. 5.3 Spearman Rank Correlation The Spearman rank correlation coefficient can be used for ordinal or rank-ordered data. To calculate the Spearman correlation for variables \\(X\\) and \\(Y\\), we first must obtain the rank for every value per variable. For this, we order the values from large to small and assign a rank to each value. Let the random variable \\(X\\) denote the age in days and \\(Y\\) denote the circumference in millimeters. Let \\(R_{xi}\\) denote the rank for the observed value \\(x_i\\) and \\(R_{yi}\\) the rank for the observed value \\(y_i\\). orange_spm &lt;- as_tibble(Orange) %&gt;% select(X = age, Y = circumference) %&gt;% mutate(X_rank = rank(X), Y_rank = rank(Y)) kable(orange_spm) X Y X_rank Y_rank 118 30 3 2.0 484 58 8 8.0 664 87 13 13.0 1004 115 18 17.5 1231 120 23 19.0 1372 142 28 23.5 1582 145 33 25.0 118 33 3 5.0 484 69 8 10.0 664 111 13 15.0 1004 156 18 26.0 1231 172 23 28.0 1372 203 28 32.5 1582 203 33 32.5 118 30 3 2.0 484 51 8 7.0 664 75 13 11.0 1004 108 18 14.0 1231 115 23 17.5 1372 139 28 21.0 1582 140 33 22.0 118 32 3 4.0 484 62 8 9.0 664 112 13 16.0 1004 167 18 27.0 1231 179 23 31.0 1372 209 28 34.0 1582 214 33 35.0 118 30 3 2.0 484 49 8 6.0 664 81 13 12.0 1004 125 18 20.0 1231 142 23 23.5 1372 174 28 29.0 1582 177 33 30.0 With the ranks, we can then calculate the difference in ranks for every observation as follows: \\[ d_i = R_{xi} - R_{yi} \\] Subsequently, we can calculate the sample Spearman correlation: \\[ r_s = 1 - \\frac{6 \\sum_{i = 1}^{n} d_{i}^{2}}{n(n^2 - 1)} \\] We can perform a hypothesis test with the following null and alternative hypotheses: \\[ H_0 : \\rho = 0 \\text{ vs. } H_a : \\rho \\neq 0 \\] Under the null hypothesis and if the sample size is large enough (\\(n \\geq 10\\)), the Spearman rank correlation coefficient is approximately normally distributed with \\(\\mu_{r_s} = 0\\) and \\(\\sigma_{r_s} = \\sqrt{\\frac{1}{n - 1}}\\). Using this, we can calculate the test statistic \\(T\\): \\[ T = \\frac{r_s - \\mu_{r_s}}{\\sigma_{r_s}} \\] We can continue with the Orange dataset: orange_spm &lt;- orange_spm %&gt;% mutate(delta = X_rank - Y_rank, delta_2 = delta^2) kable(orange_spm) X Y X_rank Y_rank delta delta_2 118 30 3 2.0 1.0 1.00 484 58 8 8.0 0.0 0.00 664 87 13 13.0 0.0 0.00 1004 115 18 17.5 0.5 0.25 1231 120 23 19.0 4.0 16.00 1372 142 28 23.5 4.5 20.25 1582 145 33 25.0 8.0 64.00 118 33 3 5.0 -2.0 4.00 484 69 8 10.0 -2.0 4.00 664 111 13 15.0 -2.0 4.00 1004 156 18 26.0 -8.0 64.00 1231 172 23 28.0 -5.0 25.00 1372 203 28 32.5 -4.5 20.25 1582 203 33 32.5 0.5 0.25 118 30 3 2.0 1.0 1.00 484 51 8 7.0 1.0 1.00 664 75 13 11.0 2.0 4.00 1004 108 18 14.0 4.0 16.00 1231 115 23 17.5 5.5 30.25 1372 139 28 21.0 7.0 49.00 1582 140 33 22.0 11.0 121.00 118 32 3 4.0 -1.0 1.00 484 62 8 9.0 -1.0 1.00 664 112 13 16.0 -3.0 9.00 1004 167 18 27.0 -9.0 81.00 1231 179 23 31.0 -8.0 64.00 1372 209 28 34.0 -6.0 36.00 1582 214 33 35.0 -2.0 4.00 118 30 3 2.0 1.0 1.00 484 49 8 6.0 2.0 4.00 664 81 13 12.0 1.0 1.00 1004 125 18 20.0 -2.0 4.00 1231 142 23 23.5 -0.5 0.25 1372 174 28 29.0 -1.0 1.00 1582 177 33 30.0 3.0 9.00 We now calculate the sample Spearman rank correlation coefficient: n &lt;- nrow(orange_spm) 1 - ((6 * sum(orange_spm$delta_2)) / (n * (n^2 - 1))) ## [1] 0.9073529 or: cor(x = orange_spm$X, y = orange_spm$Y, method = &quot;spearman&quot;) ## [1] 0.9064294 We can now continue and calculate the test statistic \\(T\\). First, we must calculate the parameters of the null distribution: \\[ \\mu_{r_s} = 0 \\] and: \\[ \\sigma_{r_s} = \\sqrt{\\frac{1}{n - 1}} = \\sqrt{\\frac{1}{35 - 1}} = 0.1714986 \\] Our test statistic \\(T\\) is: \\[ T = \\frac{r_s - \\mu_{r_s}}{\\sigma_{r_s}} = \\frac{0.9073529 - 0}{0.1714986} = 5.290731 \\] We calculate the \\(p\\)-value as follows: # Calculate the statistics for the null distribution T_mu &lt;- 0 T_se &lt;- sqrt(1 / (35 - 1)) T_stat &lt;- (0.9073529 - T_mu) / T_se p_val &lt;- pnorm(q = T_stat, lower.tail = FALSE)*2 p_val ## [1] 1.218284e-07 Or we can also calculate as follows: # Calculate the test statistic cor.test(x = orange_spm$X, y = orange_spm$Y, method = &quot;spearman&quot;, exact = F) ## ## Spearman&#39;s rank correlation rho ## ## data: orange_spm$X and orange_spm$Y ## S = 668.09, p-value = 6.712e-14 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.9064294 Using a sample dataset: # Create a sample dataset dat &lt;- tibble(X = rnorm(15), Y = rnorm(15)) dat ## # A tibble: 15 × 2 ## X Y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.396 -0.759 ## 2 0.379 0.0820 ## 3 -1.18 -0.347 ## 4 0.652 0.243 ## 5 1.47 1.68 ## 6 -1.06 0.724 ## 7 0.915 -0.711 ## 8 0.900 0.486 ## 9 0.564 0.822 ## 10 -0.376 0.492 ## 11 -1.00 0.308 ## 12 0.423 -0.867 ## 13 -0.0149 -0.0605 ## 14 -2.58 1.44 ## 15 0.318 1.46 dat &lt;- dat %&gt;% mutate(X_rank = rank(X), Y_rank = rank(Y)) %&gt;% mutate(delta = X_rank - Y_rank, delta_2 = delta^2) n &lt;- nrow(dat) 1 - ((6 * sum(dat$delta_2)) / (n * (n^2 - 1))) ## [1] -0.07142857 or: cor(x = dat$X, y = dat$Y, method = &quot;spearman&quot;) ## [1] -0.07142857 # Calculate the statistics for the null distribution T_mu &lt;- 0 T_se &lt;- sqrt(1 / (nrow(dat) - 1)) T_stat &lt;- (0.15 - T_mu) / T_se p_val &lt;- pnorm(q = T_stat, lower.tail = FALSE)*2 p_val ## [1] 0.5746281 # Calculate the test statistic cor.test(x = dat$X, y = dat$Y, method = &quot;spearman&quot;, exact = F) ## ## Spearman&#39;s rank correlation rho ## ## data: dat$X and dat$Y ## S = 600, p-value = 0.8003 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## -0.07142857 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
