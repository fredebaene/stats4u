[["logistic-regression.html", "3 Logistic Regression 3.1 Ordinary Logistic Regression 3.2 Conditional Logistic Regression", " 3 Logistic Regression In this chapter, we explore logistic regression. This type of model can be used to estimate the probability of a particular event occurring or not. This means that the outcome must be binary, i.e., there are only two possible outcomes. 3.1 Ordinary Logistic Regression The most well-known logistic regression is ordinary logistic regression. In a next chapter, we will discuss conditional logistic regression. 3.1.1 Context Suppose we randomly select a patient and assess his/her survival status. The outcome of this random experiment is captured using the random variable \\(Y\\). The random variable \\(Y\\) can take on one of only two possible outcomes, therefore it is a binary random variable (or Bernoulli random variable). If we repeat this process for \\(n\\) patients, then we have \\(n\\) random variables, i.e., \\(Y_i\\) for \\(i = 1, 2, ..., n\\). Assume we observe the following: ## i Y ## 1 1 1 ## 2 2 0 ## 3 3 0 ## 4 4 1 ## 5 5 0 We have \\(n = 5\\) patients. For each patient, the outcome \\(Y_i\\) is observed. Because the variable \\(Y_i\\) is a Bernoulli random variable, its probability distribution is defined by the following probability mass function: \\[ f_i (Y_i) = \\pi_i^{Y_i} \\cdot (1 - \\pi_i)^{(1 - Y_i)} \\] We again observe the previously observed outcomes but recognize that, for each patient, the outcome is drawn randomly from a Bernoulli distribution defined by \\(\\pi_i\\): ## i Y P.Y...1. ## 1 1 1 0.2875775 ## 2 2 0 0.7883051 ## 3 3 0 0.4089769 ## 4 4 1 0.8830174 ## 5 5 0 0.9404673 We can also visualize the five Bernoulli distributions for each of the patients. for (idx in seq_along(probs)) { prob &lt;- probs[idx] plot( x = c(0, 1), y = c(1 - prob, prob), xlab = &quot;Y&quot;, ylab = &quot;Probability&quot;, main = paste0(&quot;Patient 0&quot;, idx), ylim = c(0, 1), col = &quot;red&quot;, pch = 16 ) } 3.1.2 Model Fitting We use the heart dataset and denote the outcome in the heart dataset with the variable \\(Y\\), with \\(Y = 1\\) if the patient has a heart disease and \\(Y = 0\\) otherwise. heart$Y &lt;- as.integer(heart$HeartDisease) Prior to fitting a model, we cast the categorical variables to factors. heart$ChestPainType &lt;- factor( x = heart$ChestPainType, levels = c(&quot;ASY&quot;, &quot;NAP&quot;, &quot;TA&quot;, &quot;ATA&quot;) ) heart$Sex &lt;- factor( x = heart$Sex, levels = c(&quot;F&quot;, &quot;M&quot;) ) We fit the following model to the data: \\[ \\mathbb{E} [Y_i] = \\beta_0 + \\beta_1 \\cdot X_{age} + \\beta_2 \\cdot X_{male} \\] with \\(X_{age}\\) the age and \\(X_{male} = 1\\) if the person is male and \\(X_{male} = 0\\) otherwise. The model contains three parameters, i.e., \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\). We call glm() as follows to fit the model: model &lt;- glm( formula = &quot;HeartDisease ~ Age + Sex&quot;, data = heart, family = binomial(link = &quot;logit&quot;) ) ## ## Call: ## glm(formula = &quot;HeartDisease ~ Age + Sex&quot;, family = binomial(link = &quot;logit&quot;), ## data = heart) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.634893 0.481185 -9.632 &lt; 2e-16 *** ## Age 0.066531 0.008165 8.148 3.69e-16 *** ## SexM 1.641195 0.189345 8.668 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1262.1 on 917 degrees of freedom ## Residual deviance: 1101.6 on 915 degrees of freedom ## AIC: 1107.6 ## ## Number of Fisher Scoring iterations: 4 3.1.3 Interpretation Us the parameter estimates, we have the following model: \\[ log(\\frac{\\pi_i}{1 - \\pi_i}) = -4.634893 + 0.066531 \\cdot X_{age} + 1.641195 \\cdot X_{male} \\] Assume that person \\(i\\) is a 40-year-old woman. The probability of person \\(i\\) having a heart disease is the following: \\[ \\begin{align} log(\\frac{\\pi_i}{1 - \\pi_i}) &amp;= -4.634893 + 0.066531 \\cdot 40 + 1.641195 \\cdot 0 \\\\ &amp;= -4.634893 + 0.066531 \\cdot 40 \\\\ &amp;= -1.973653 \\end{align} \\] The estimated logit equals -1.973653. The estimated odds of a 40-year-old woman having a heart disease is 0.1389483. This means that the probability of a 40-year-old woman having a heart disease is 0.1389483 times smaller than the probability of a 40-year-old woman not having a heart disease. \\[ \\begin{align} log(\\frac{\\pi_i}{1 - \\pi_i}) &amp;= -1.973653 \\\\ &amp;\\leftrightarrow \\frac{\\pi_i}{1 - \\pi_i} = exp(-1.973653) \\\\ &amp;\\leftrightarrow \\frac{\\pi_i}{1 - \\pi_i} = 0.1389483 \\end{align} \\] The estimated odds of a 40-year-old woman having a heart disease can be used to estimate the probability of such a person having a heart disease. \\[ \\begin{align} log(\\frac{\\pi_i}{1 - \\pi_i}) &amp;\\leftrightarrow \\pi_i = \\frac{exp(-1.973653)}{1 + exp(-1.973653)} \\\\ &amp;\\leftrightarrow \\pi_i = \\frac{0.1389483}{1 + 0.1389483} \\\\ &amp;\\leftrightarrow \\pi_i = \\frac{0.1389483}{1.1389483} \\\\ &amp;\\leftrightarrow \\pi_i = 0.121997 \\end{align} \\] Assume person \\(j\\) is a 40-year-old man. We can now also calculate the estimated odds of having a heart disease. \\[ \\begin{align} log(\\frac{\\pi_j}{1 - \\pi_j}) &amp;= -4.634893 + 0.066531 \\cdot 40 + 1.641195 \\cdot 1 \\\\ &amp;\\leftrightarrow \\frac{\\pi_j}{1 - \\pi_j} = exp(-4.634893 + 0.066531 \\cdot 40 + 1.641195) \\\\ &amp;\\leftrightarrow \\frac{\\pi_j}{1 - \\pi_j} = exp(-4.634893 + 0.066531 \\cdot 40) \\cdot exp(1.641195) \\\\ &amp;\\leftrightarrow \\frac{\\pi_j}{1 - \\pi_j} = 0.1389483 \\cdot 5.161334 \\\\ &amp;\\leftrightarrow \\frac{\\pi_j}{1 - \\pi_j} = 0.7171586 \\end{align} \\] From this, we can also see the following: \\[ \\begin{align} log(\\frac{\\pi_j}{1 - \\pi_j}) &amp;= -4.634893 + 0.066531 \\cdot 40 + 1.641195 \\cdot 1 \\\\ &amp;\\leftrightarrow \\frac{\\pi_j}{1 - \\pi_j} = exp(-4.634893 + 0.066531 \\cdot 40 + 1.641195) \\\\ &amp;\\leftrightarrow \\frac{\\pi_j}{1 - \\pi_j} = exp(-4.634893 + 0.066531 \\cdot 40) \\cdot exp(1.641195) \\\\ &amp;\\leftrightarrow exp(1.641195) = \\frac{\\frac{\\pi_j}{1 - \\pi_j}}{exp(-4.634893 + 0.066531 \\cdot 40)} \\\\ &amp;\\leftrightarrow exp(1.641195) = \\frac{\\frac{\\pi_j}{1 - \\pi_j}}{\\frac{\\pi_i}{1 - \\pi_i}} \\end{align} \\] In other words, the estimate for \\(\\beta_2\\) can be exponentiated to obtain an odds ratio, and more specifically the odds ratio of the odds of a 40-year-old man having a heart disease vs. a 40-year-old woman having a heart disease. 3.1.4 Maximum Likelihood Estimation The model parameters are estimated using the maximum likelihood estimation framework. We let the random variable \\(Y\\) denote the outcome, with \\(Y = 1\\) indicating that the event of interest occurred and \\(Y = 0\\) that the event of interest did not occur. Because the variable \\(Y\\) is a binary variable its probability distribution can be described using the following probability mass function: \\[ \\mathbb{P} (Y = y) = \\pi^{y} \\cdot (1 - \\pi)^{(1 - y)} \\] with \\(\\pi\\) representing the probability of \\(Y = 1\\). We can verify this: \\[ \\mathbb{P} (Y = 1) = \\pi^{1} \\cdot (1 - \\pi)^{(1 - 1)} = \\pi \\cdot (1 - \\pi)^0 = \\pi \\] and \\[ \\mathbb{P} (Y = 0) = \\pi^0 \\cdot (1 - \\pi)^{(1 - 0)} = (1 - \\pi)^1 = 1 - \\pi \\] This distribution is defined by a single parameter \\(\\pi\\). In the simple logistic regression model, the probability of the event of interest occurring \\(\\pi\\) is linked to the linear predictor as follows: \\[ \\pi_i = \\frac{exp(\\beta_0 + \\beta_1 x_i)}{1 + exp(\\beta_0 + \\beta_1 x_i)} \\] In other words, we have: \\[ log(\\frac{\\pi_i}{1 - \\pi_i}) = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] We can now derive the likelihood and log-likelihood function. The likelihood function is: \\[ \\begin{align} L(\\beta) &amp;= \\prod_{i = 1}^{n} \\pi_i^{y_i} (1 - \\pi_i)^{(1 - y_i)} \\end{align} \\] The log-likelihood function is: \\[ \\begin{align} l(\\beta) = log(L(\\beta)) &amp;= log \\left( \\prod_{i = 1}^{n} \\pi_i^{y_i} (1 - \\pi_i)^{(1 - y_i)} \\right) \\\\ &amp;= \\sum_{i = 1}^{n} \\left[ log(\\pi_i^{y_i} (1 - \\pi_i)^{(1 - y_i)}) \\right] \\\\ &amp;= \\sum_{i = 1}^{n} \\left[ y_i \\cdot log(\\pi_i) + (1 - y_i) \\cdot log(1 - \\pi_i) \\right] \\\\ &amp;= \\sum_{i = 1}^{n} \\left[ y_i \\cdot log(\\pi_i) - y_i \\cdot log(1 - \\pi_i) + log(1 - \\pi_i) \\right] \\\\ &amp;= \\sum_{i = 1}^{n} \\left[ y_i \\cdot log(\\frac{\\pi_i}{1 - \\pi_i}) + log(1 - \\pi_i) \\right] \\end{align} \\] We can derive the following: \\[ \\begin{align} 1 - \\pi &amp;= 1 - \\frac{exp(\\beta_0 + \\beta_1 x_i)}{1 + exp(\\beta_0 + \\beta_1 x_i)} \\\\ &amp;= \\frac{1 + exp(\\beta_0 + \\beta_1 x_i)}{1 + exp(\\beta_0 + \\beta_1 x_i)} - \\frac{exp(\\beta_0 + \\beta_1 x_i)}{1 + exp(\\beta_0 + \\beta_1 x_i)} \\\\ &amp;= \\frac{1}{1 + exp(\\beta_0 + \\beta_1 x_i)} \\\\ &amp;= (1 + exp(\\beta_0 + \\beta_1 x_i))^{-1} \\end{align} \\] This means the last expression for the log-likelihood can be simplified: \\[ \\begin{align} l(\\beta) = log(L(\\beta)) &amp;= \\sum_{i = 1}^{n} \\left[ y_i \\cdot log(\\frac{\\pi_i}{1 - \\pi_i}) + log(1 - \\pi_i) \\right] \\\\ &amp;= \\sum_{i = 1}^{n} \\bigg[ y_i \\cdot (\\beta_0 + \\beta_1 x_i) - log(1 + exp(\\beta_0 + \\beta_1 x_i)) \\bigg] \\end{align} \\] The maximum likelihood estimates are those values for the model parameters that minimize the log-likelihood. We can manually look for these maximum likelihood estimates. 3.2 Conditional Logistic Regression In this chapter, we explore conditional logistic regression. Conditional logistic regression allows us to not have to estimate nuisance parameters. We will illustrate this with an example. Suppose we have a dataset comprising data on 20 patients distributed across two hospitals (with 10 patients each). For each patient, we know what hospital they were admitted to, their age at the time of hospital admission, and if they survived their hospital stay. # Create the dataset set.seed(345) df &lt;- tibble::tibble( i = 1L:20L, hospital = factor(x = rep(c(&quot;A&quot;, &quot;B&quot;), each = 10), levels = c(&quot;A&quot;, &quot;B&quot;)), age = c(65, 70, 75, 60, 68, 72, 80, 78, 85, 76, 55, 60, 65, 70, 75, 80, 85, 90, 72, 78), Y = c(1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1) ) ## # A tibble: 20 × 4 ## i hospital age Y ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 A 65 1 ## 2 2 A 70 0 ## 3 3 A 75 1 ## 4 4 A 60 0 ## 5 5 A 68 1 ## 6 6 A 72 0 ## 7 7 A 80 0 ## 8 8 A 78 0 ## 9 9 A 85 0 ## 10 10 A 76 0 ## 11 11 B 55 1 ## 12 12 B 60 1 ## 13 13 B 65 1 ## 14 14 B 70 0 ## 15 15 B 75 1 ## 16 16 B 80 1 ## 17 17 B 85 0 ## 18 18 B 90 1 ## 19 19 B 72 0 ## 20 20 B 78 1 3.2.1 Definition Let \\(Y_i\\) denote the outcome for patient \\(i\\), with \\(Y_i = 1\\) if patient \\(i\\) died and \\(Y_i = 0\\) otherwise. Let \\(X_i\\) denote the age of patient \\(i\\) at the time of hospital admission. Let \\(Z_i\\) denote the hospital at which patient \\(i\\) was admitted, with \\(Z_i = 0\\) if patient \\(i\\) was admitted to hospital A and \\(Z_i = 1\\) otherwise. We can now calculate the probability of patient \\(i\\) dying conditional on their age and the hospital where they were admitted to as: \\[ \\mathbb{P} (Y_i = 1) = \\frac{exp(\\beta_z \\cdot z_i + \\beta_x \\cdot x_i)}{1 + exp(\\beta_z \\cdot z_i + \\beta_x \\cdot x_i)} \\] We know that \\(\\beta_z = 0.24\\) and \\(\\beta_x = 0.5\\). We can now calculate the probability of dying for each patient. # Define the parameters B_z &lt;- 1.85 B_x &lt;- 0.05 # Calculate the probability of dying per patient df &lt;- df %&gt;% dplyr::rowwise() %&gt;% dplyr::mutate(Z_i = dplyr::if_else(hospital == &quot;A&quot;, 0, 1), X_i = age) %&gt;% dplyr::mutate(P_i = exp(Z_i * B_z + X_i * B_x) / (1 + exp(Z_i * B_z + X_i * B_x))) ## # A tibble: 20 × 7 ## # Rowwise: ## i hospital age Y Z_i X_i P_i ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 A 65 1 0 65 0.963 ## 2 2 A 70 0 0 70 0.971 ## 3 3 A 75 1 0 75 0.977 ## 4 4 A 60 0 0 60 0.953 ## 5 5 A 68 1 0 68 0.968 ## 6 6 A 72 0 0 72 0.973 ## 7 7 A 80 0 0 80 0.982 ## 8 8 A 78 0 0 78 0.980 ## 9 9 A 85 0 0 85 0.986 ## 10 10 A 76 0 0 76 0.978 ## 11 11 B 55 1 1 55 0.990 ## 12 12 B 60 1 1 60 0.992 ## 13 13 B 65 1 1 65 0.994 ## 14 14 B 70 0 1 70 0.995 ## 15 15 B 75 1 1 75 0.996 ## 16 16 B 80 1 1 80 0.997 ## 17 17 B 85 0 1 85 0.998 ## 18 18 B 90 1 1 90 0.998 ## 19 19 B 72 0 1 72 0.996 ## 20 20 B 78 1 1 78 0.997 3.2.2 Probability In this section, we outline how to get the probability of a particular outcome occurring. Suppose we have an ICU with five patients. For each patient, the presence of diabetes as an underlying comorbidity is recorded. Let \\(X = 1\\) denote that the patient has diabetes and \\(X = 0\\) otherwise. The probability of a patient having diabetes depends on several factors, such as age, sex, weight, lifestyle, genetics, etc, and therefore this probability differs between patients. # Construct a sample dataset probs &lt;- tibble::tibble(i = 1L:5L, P_i = runif(n = 5)) In the following table, we see that the probability that patient \\(i = 1\\) has diabetes is \\(\\mathbb{P} (X_1 = 1) = 0.312\\) and for patient \\(i = 4\\) it is \\(\\mathbb{P} (X_4 = 1) = 0.887\\). ## # A tibble: 5 × 2 ## i P_i ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.312 ## 2 2 0.582 ## 3 3 0.638 ## 4 4 0.887 ## 5 5 0.248 We can also calculate the probability of a patient not having diabetes. For this, the following probability mass function can be used: \\[ f(Y_i = y_1, \\pi_i) = \\pi_i^{y_i} \\cdot (1 - \\pi_i)^{(1 - y_i)} \\] We can verify this: \\[ f(Y_4 = 1, \\pi_4) = 0.887^1 * (1 - 0.887)^{(1 - 1)} = 0.887 \\] and \\[ f(Y_4 = 0, \\pi_4) = 0.887^0 * (1 - 0.887)^{(1 - 0)} = 0.113 \\] Let the random variable \\(Y\\) denote the number of patients in the ICU having diabetes. First, we must construct the sample space. One example of a possible outcome in the sample space is: \\[ (X_1 = 1, X_2 = 0, X_3 = 0, X_4 = 0, X_5 = 1) \\] This possible outcome maps to \\(Y = 2\\). Another example of a possible outcome: \\[ (X_1 = 0, X_2 = 1, X_3 = 1, X_4 = 0, X_5 = 1) \\] This possible outcome maps to \\(Y = 3\\). Another example of a possible outcome: \\[ (X_1 = 0, X_2 = 0, X_3 = 1, X_4 = 1, X_5 = 0) \\] This possible outcome maps to \\(Y = 2\\). Note that we now have two possible outcomes in the sample space that map to \\(Y = 2\\). The sample space contains 32 possible outcomes: \\[ {5 \\choose 0} + {5 \\choose 1} + {5 \\choose 2} + {5 \\choose 3} + {5 \\choose 4} + {5 \\choose 5} = 32 \\] The following are five examples of the sample space: ## # A tibble: 5 × 5 ## Patient_1 Patient_2 Patient_3 Patient_4 Patient_5 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 0 ## 2 0 0 0 0 1 ## 3 0 0 0 1 0 ## 4 0 0 0 1 1 ## 5 0 0 1 0 0 We can now calculate the probability of each possible outcome occurring. For the possible outcome \\[ (X_1 = 0, X_2 = 1, X_3 = 1, X_4 = 0, X_5 = 1) \\] we have \\[ \\begin{aligned} &amp; \\mathbb{P} (Y_1 = 0) \\cdot \\mathbb{P} (Y_2 = 1) \\cdot \\mathbb{P} (Y_3 = 1) \\cdot \\mathbb{P} (Y_4 = 0) \\cdot \\mathbb{P} (Y_5 = 1) \\\\ &amp;= \\prod_{i = 1}^{5} \\bigg[ f(Y_i = y_i, \\pi_i) \\bigg] \\\\ &amp;= f(Y_1 = 0, \\pi_1) \\cdot f(Y_2 = 1, \\pi_2) \\cdot f(Y_3 = 1, \\pi_3) \\cdot f(Y_4 = 0, \\pi_4) \\cdot f(Y_5 = 1, \\pi_5) \\\\ &amp;= (1 - 0.3121850) \\cdot 0.5821874 \\cdot 0.6381059 \\cdot (1 - 0.8872664) \\cdot 0.2481097 \\\\ &amp;= 0.007147012 \\end{aligned} \\] We can calculate this as follows: # Get the observed outcome Y_vector &lt;- c(0, 1, 1, 0, 1) # Calculate the probability prob_Y &lt;- prod(probs$P_i^Y_vector * (1 - probs$P_i)^(1 - Y_vector)) prob_Y ## [1] 0.007147012 This is the probability of this possible outcome occurring. This possible outcome maps to \\(Y = 3\\). If we want to know \\(\\mathbb{P} (Y = 3)\\), we must sum the probabilities of every possible outcome that maps to \\(Y = 3\\). We define the set of all possible outcomes mapping to \\(Y = 3\\) as \\(S(t)\\) where \\(t = 3\\): \\[ S(t) = \\{ (Y_1 = y_1^*, ..., Y_5 = y_5^*) | \\sum_{i = 1}^{5} y_i^* = t \\} \\] In the case of \\(Y = 3\\), \\(S(3)\\) contains ten possible outcomes. We calculate the probability of \\(Y = 3\\) as follows: \\[ \\mathbb{P} (Y = 3) = \\sum_{S(3)} \\Bigg[ \\prod_{i = 1}^{5} \\bigg[ f(Y_i = y_i^*, \\pi_i) \\bigg] \\Bigg] \\] We can calculate this as follows: # Get the subset of the sample space that maps to Y = 3 event_space &lt;- sample_space[rowSums(sample_space) == 3,, drop = F] # Calculate the probability prob_Y &lt;- 0.0 for (i in 1:nrow(event_space)) { # Get the observed outcome Y_vector &lt;- as.integer(event_space[i,, drop = TRUE]) prob_Y &lt;- prob_Y + prod((probs$P_i)^Y_vector * (1 - probs$P_i)^(1 - Y_vector)) } ## [1] 0.3736777 So, we have \\(\\mathbb{P} (Y = 3) = 0.3736777\\). As stated earlier, the probability of a patient \\(i\\) having diabetes depends on several factors, such as sex, age, weight, etc. The probability of a patient \\(i\\) having diabetes is denoted as \\(\\pi_i = \\mathbb{P} (Y_i = 1)\\). This probability can be derived as follows: \\[ \\pi_i = \\frac{exp(\\alpha + \\beta \\textbf{X}^{-1})}{1 + exp(\\alpha + \\beta \\textbf{X}^{-1})} \\] We can use this to rewrite the following: \\[ \\begin{align} f(Y_i = y_i, \\pi_i) &amp;= \\pi_i^{y_i} * (1 - \\pi_i)^{(1 - y_i)} \\\\ &amp;= \\frac{exp(y_i \\cdot (\\alpha + \\beta \\textbf{X}^{-1}))}{1 + exp(\\alpha + \\beta \\textbf{X}^{-1})} \\end{align} \\] For a particular possible outcome of the sample space, we have: \\[ \\begin{align} \\prod_{i = 1}^5 f(Y_i = y_i, \\pi_i) &amp;= \\prod_{i = 1}^5 \\frac{exp(y_i \\cdot (\\alpha + \\beta \\textbf{X}^{-1}))}{1 + exp(\\alpha + \\beta \\textbf{X}^{-1})} \\\\ &amp;= \\frac{\\prod_{i = 1}^5 exp(y_i \\cdot (\\alpha + \\beta \\textbf{X}^{-1}))}{\\prod_{i = 1}^5 \\big[ 1 + exp(\\alpha + \\beta \\textbf{X}^{-1}) \\big]} \\\\ &amp;= \\frac{\\prod_{i = 1}^5 exp(y_i \\alpha + y_i \\beta \\textbf{X}^{-1}))}{\\prod_{i = 1}^5 \\big[ 1 + exp(\\alpha + \\beta \\textbf{X}^{-1}) \\big]} \\\\ &amp;= \\frac{exp(\\sum_{i = 1}^5 \\big[ y_i \\alpha + y_i \\beta \\textbf{X}^{-1} \\big] )}{\\prod_{i = 1}^5 \\big[ 1 + exp(\\alpha + \\beta \\textbf{X}^{-1}) \\big]} \\\\ &amp;= \\frac{exp(\\sum_{i = 1}^5 y_i \\alpha + \\sum_{i = 1}^5 y_i \\beta \\textbf{X}^{-1})}{\\prod_{i = 1}^5 \\big[ 1 + exp(\\alpha + \\beta \\textbf{X}^{-1}) \\big]} \\\\ &amp;= \\frac{exp(\\alpha \\sum_{i = 1}^5 y_i + \\sum_{i = 1}^5 y_i \\beta \\textbf{X}^{-1})}{\\prod_{i = 1}^5 \\big[ 1 + exp(\\alpha + \\beta \\textbf{X}^{-1}) \\big]} \\\\ \\end{align} \\] 3.2.3 Conditional Probability We can calculate the probability \\(\\mathbb{P} (Y = y)\\). But we can also calculate the conditional probability of a particular possible outcome given the event of interest. Suppose we are not interested in the following: \\[ \\mathbb{P} (X_1 = 0, X_2 = 1, X_3 = 1, X_4 = 0, X_5 = 1) \\] but in the following: \\[ \\mathbb{P} (X_1 = 0, X_2 = 1, X_3 = 1, X_4 = 0, X_5 = 1 | Y = 3) \\] We can calculate this as follows: \\[ \\begin{align} \\mathbb{P} (X_1 = 0, X_2 = 1, X_3 = 1, X_4 = 0, X_5 = 1 | Y = 3) &amp;= \\frac{\\mathbb{P} (X_1 = 0, X_2 = 1, X_3 = 1, X_4 = 0, X_5 = 1)}{\\mathbb{P} (Y = 3)} \\\\ &amp;= \\frac{\\mathbb{P} (X_1 = 0, X_2 = 1, X_3 = 1, X_4 = 0, X_5 = 1)}{\\sum_{S(3)} \\Bigg[ \\prod_{i = 1}^{5} \\bigg[ f(Y_i = y_i^*, \\pi_i) \\bigg] \\Bigg]} \\end{align} \\] 3.2.4 Probability Before continuing our explanation on conditional logistic regression, we focus on how to calculate the probability of a particular outcome occurring. For this, we first focus on a single hospital, i.e., we focus on hospital A. ## # A tibble: 10 × 7 ## # Rowwise: ## i hospital age Y Z_i X_i P_i ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 A 65 1 0 65 0.963 ## 2 2 A 70 0 0 70 0.971 ## 3 3 A 75 1 0 75 0.977 ## 4 4 A 60 0 0 60 0.953 ## 5 5 A 68 1 0 68 0.968 ## 6 6 A 72 0 0 72 0.973 ## 7 7 A 80 0 0 80 0.982 ## 8 8 A 78 0 0 78 0.980 ## 9 9 A 85 0 0 85 0.986 ## 10 10 A 76 0 0 76 0.978 Hospital A has 10 patients, and there are two possible outcomes for each patient, dead or alive. The outcome of interest \\(Y\\) is the number of patients that have died during their hospital stay. The data frame above shows one possible outcome for hospital A where two out of ten patients died during their hospital stay. More specifically, patients \\(i = 1\\) and \\(i = 5\\) died. This is only one of a few possible outcomes where two patients die. There are, in fact, \\(10 \\choose 2 = 45\\) possible outcomes in which two patients die. The total number of possible outcomes is shown below, where \\(Y\\) denotes how many patients died during their hospital stay and \\(SS\\) denotes how many of the possible outcomes in the sample space map to \\(Y\\). tibble::tibble( Y = 0L:10L, SS = choose(10, Y) ) ## # A tibble: 11 × 2 ## Y SS ## &lt;int&gt; &lt;dbl&gt; ## 1 0 1 ## 2 1 10 ## 3 2 45 ## 4 3 120 ## 5 4 210 ## 6 5 252 ## 7 6 210 ## 8 7 120 ## 9 8 45 ## 10 9 10 ## 11 10 1 So, with \\(Y = 9\\), there are 9 possible outcomes mapping to this outcome. We can define the set of possible outcomes mapping to \\(Y = 9\\) as follows: \\[ S(t) = \\{ (Y_1 = y_1^*, ..., Y_{10} = y_{10}^*) | \\sum_{i = 1}^{10} y_i^* = t \\} \\] with \\(t = y\\). In this case, \\(S(t)\\) denotes the set of possible outcomes mapping to \\(Y = t\\). If we know the probability of each patient \\(i\\) dying, then, for each possible outcome we can calculate the probability of that outcome occurring as follows: \\[ \\begin{align} \\mathbb{P} (\\textbf{Y} = \\textbf{y}) = \\mathbb{P} (Y_1 = y_1, ..., Y_{10} = y_{10}) &amp;= \\mathbb{P} (Y_1 = y_1) \\times \\text{ ... } \\times\\mathbb{P} (Y_{10} = y_{10}) \\end{align} \\] If we want to know the probability of \\(Y = 3\\), then we must first obtain the probabilities for each possible outcome in \\(S(3)\\) and sum these up. So we have: \\[ \\sum_{S(t)} \\mathbb{P} (\\textbf{Y} = \\textbf{y} | t = 3) \\] We can calculate this: # Construct the sample space sample_space &lt;- gtools::permutations( n = 2, r = 10, v = c(0, 1), repeats.allowed = TRUE ) sample_space &lt;- tibble::as_tibble(sample_space, .name_repair = &quot;minimal&quot;) # Get the subset of possible outcomes corresponding to Y = 3 event_space &lt;- sample_space[rowSums(sample_space) == 3,, drop = F] # Calculate the probability of Y = 3 prob_Y &lt;- 0.0 probs &lt;- df %&gt;% dplyr::filter(hospital == &quot;A&quot;) %&gt;% dplyr::pull(P_i) for (i in 1L:nrow(event_space)) { # Get the outcomes for each patient corresponding to Y = 3 Y &lt;- as.integer(event_space[i,, drop = TRUE]) # Calculate the probability of this particular outcome occurring prob_Y &lt;- prob_Y + prod(probs^Y * (1 - probs)^(1 - Y)) } prob_Y ## [1] 8.732308e-10 \\[ \\mathbb{P} (\\textbf{Y} = \\textbf{y}) = \\mathbb{P} (Y_1 = y_1, Y_2 = y_2, ..., Y_n = Y_n) &amp;= \\frac{exp(\\beta_z \\cdot z_i + \\beta_x \\cdot x_i)}{1 + exp(\\beta_z \\cdot z_i + \\beta_x \\cdot x_i)} \\] Now, suppose we are only interested in \\(\\beta_x\\) and not in \\(\\beta_z\\), the latter being a nuisance parameter. We would like to eliminate \\(\\beta_z\\) from the estimation. We can do this by conditioning the likelihood function. This results in an estimate only for age \\(X_i\\). Within each hospital, we want to calculate the probability of obtaining a particular outcome, e.g., 3 out of 10 patients died. Let us look at our two hospitals and the outcomes. We can then see that 3 out of 10 patients died in hospital A and 7 out of 10 patients in hospital B. table(df$hospital, df$Y) ## ## 0 1 ## A 7 3 ## B 3 7 We can calculate the probability of this outcome for each hospital. To calculate the probability of a single patient dying, we have: \\[ \\mathbb{P} (Y_i = 1) = \\frac{exp(\\beta_z z_i + \\beta_x x_i)}{1 + exp(\\beta_z z_i + \\beta_x x_i)} \\] Or if we do not know what the outcome is but we want to calculate the probability of that outcome: \\[ \\mathbb{P} (Y_i = y_i) = \\frac{exp(y_i \\cdot (\\beta_z z_i + \\beta_x x_i))}{1 + exp(\\beta_z z_i + \\beta_x x_i)} \\] We have ten patients per hospital. If we want to calculate the probability of a particular outcome occurring, we have: \\[ \\begin{align} \\mathbb{P} (Y_1 = y_1, ..., Y_n = y_n) &amp;= \\prod_{i = 1}^{n} \\frac{exp(y_i \\cdot (\\beta_z z_i + \\beta_x x_i))}{1 + exp(\\beta_z z_i + \\beta_x x_i)} \\\\ &amp;= \\frac{\\prod_{i = 1}^{n} exp(y_i \\cdot (\\beta_z z_i + \\beta_x x_i))}{\\prod_{i = 1}^{n} [1 + exp(\\beta_z z_i + \\beta_x x_i)]} \\\\ &amp;= \\frac{\\prod_{i = 1}^{n} exp(y_i \\beta_z z_i + y_i \\beta_x x_i))}{\\prod_{i = 1}^{n} [1 + exp(\\beta_z z_i + \\beta_x x_i)]} \\\\ &amp;= \\frac{exp(\\sum_{i = 1}^{n} y_i \\beta_z z_i + \\sum_{i = 1}^{n} y_i \\beta_x x_i))}{\\prod_{i = 1}^{n} [1 + exp(\\beta_z z_i + \\beta_x x_i)]} \\\\ &amp;= \\frac{exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i))}{\\prod_{i = 1}^{n} [1 + exp(\\beta_z z_i + \\beta_x x_i)]} \\end{align} \\] We can now calculate the probability of the outcome \\(Y_1 = 1\\), \\(Y_3 = 1\\), and \\(Y_8 = 1\\) in hospital A occurring. Note that we specify three particular patients that have died. But if we are only interested in 3 out of 10 patients dying in hospital A. In that case, there are \\({10 \\choose 3} = 120\\) outcomes of interest. First, we define this set of 120 outcomes: \\[ S(t) = \\{ (Y_1 = y_1, Y_2, = y_2, ..., Y_n = y_n) | \\sum_{i = 1}^{n} y_i = t \\} \\] To calculate the probability of \\(S(t)\\) occurring, we can calculate this as follows: \\[ \\begin{align} \\mathbb{P} (S(t)) &amp;= \\sum_{S(t)} \\mathbb{P} (Y_1 = y_i, ..., Y_n = y_n) \\\\ &amp;= \\sum_{S(t)} [\\frac{exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i))}{\\prod_{i = 1}^{n} [1 + exp(\\beta_z z_i + \\beta_x x_i)]}] \\\\ &amp;= \\frac{1}{\\prod_{i = 1}^{n} [1 + exp(\\beta_z z_i + \\beta_x x_i)]} \\cdot \\sum_{S(t)} [exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i))] \\end{align} \\] We can now calculate the conditional probability of the outcome \\(Y_1 = 1\\), \\(Y_3 = 1\\), and \\(Y_8\\) conditional on that 3 out of 10 patients died in hospital A. \\[ \\begin{align} \\mathbb{P} (Y_1 = 0, Y_2 = 0, Y_3 = 1, ..., Y_{10} = 0 | \\sum_{i} y_i = 10) &amp;= \\frac{\\mathbb{P} (Y_1 = y_1, ..., Y_{10} = y_{10})}{\\mathbb{P} (S(t = 3))} \\\\ &amp;= \\frac{\\frac{exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i)}{\\prod_{i = 1}^{n} [1 + exp(\\beta_z z_i + \\beta_x x_i)]}}{\\frac{1}{\\prod_{i = 1}^{n} [1 + exp(\\beta_z z_i + \\beta_x x_i)]} \\cdot \\sum_{S(t)} [exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i))]} \\\\ &amp;= \\frac{exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i)}{\\sum_{S(t)} [exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i)]} \\end{align} \\] Because we look at each hospital individually, we know what the value is for \\(z_i\\) for each patient. If we look at hospital B, we know that \\(z_i = 1\\) for \\(i = 1, ..., 10\\). In that case, we have: \\[ \\begin{align} \\mathbb{P} (Y_1 = 0, Y_2 = 0, Y_3 = 1, ..., Y_{10} = 0 | \\sum_{i} y_i = 10) &amp;= \\frac{exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i)}{\\sum_{S(t)} [exp(\\beta_z \\sum_{i = 1}^{n} y_i z_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i)]} \\\\ &amp;= \\frac{exp(\\beta_z \\sum_{i = 1}^{n} y_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i)}{\\sum_{S(t)} [exp(\\beta_z \\sum_{i = 1}^{n} y_i + \\beta_x \\sum_{i = 1}^{n} y_i x_i)]} \\\\ &amp;= \\frac{exp(\\beta_z t + \\beta_x \\sum_{i = 1}^{n} y_i x_i)}{\\sum_{S(t)} [exp(\\beta_z t + \\beta_x \\sum_{i = 1}^{n} y_i x_i)]} \\\\ &amp;= \\frac{exp(\\beta_z t) exp(\\beta_x \\sum_{i = 1}^{n} y_i x_i)}{\\sum_{S(t)} [exp(\\beta_z t) exp(\\beta_x \\sum_{i = 1}^{n} y_i x_i)]} \\\\ &amp;= \\frac{exp(\\beta_z t) exp(\\beta_x \\sum_{i = 1}^{n} y_i x_i)}{exp(\\beta_z t) \\sum_{S(t)} [exp(\\beta_x \\sum_{i = 1}^{n} y_i x_i)]} \\\\ &amp;= \\frac{exp(\\beta_x \\sum_{i = 1}^{n} y_i x_i)}{\\sum_{S(t)} [exp(\\beta_x \\sum_{i = 1}^{n} y_i x_i)]} \\end{align} \\] This is the conditional probability. We can use this to determine the conditional likelihood and log-likelihood, in which we do not have to estimate the nuisance parameter \\(\\beta_z\\). "],["relationships-and-regression.html", "5 Relationships and Regression 5.1 Relationships 5.2 Pearson Correlation Coefficient 5.3 Spearman Rank Correlation", " 5 Relationships and Regression 5.1 Relationships Two random variables are related if changes in one variable are consistently associated with changes in another variable. Or, in other words, when the two random variables tend to change together. We say that the two variables have a relationship (or relation). When describing relationships between variables, we must take into account (1) the pattern, (2) the direction, and (3) the strength of the relationship. The pattern of the relationship focuses on the shape of the regression curve of the relationship. This can be linear, quadratic, exponential, etc. The relationship can be positive or negative (direction). And the relationship can be strong or weak. We will come back on the topic of strength of a relationship when discussing the Pearson correlation coefficient. As an example, let us have a look at the Orange dataset. This dataset contains 35 observations on two random variables: age in days and circumference in millimeters. In the following figure, we see that an increase in age tends to be associated with an increase in circumference. Or, in other words, relatively low values for age tend to occur with relatively low values for circumference, and relatively high values for age tend to occur with relatively high values for circumference. This indicates a positive relationship. When drawing a smoother through the data, it seems the relationship can be described with an approximately straight line, making it a linear relationship. ggplot(Orange, aes(x = age, y = circumference)) + geom_point() + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;, se = FALSE) + labs(x = &quot;Age (days)&quot;, y = &quot;Circumference (mm)&quot;) + theme_classic() In the following sections, we will focus on statistical measures that can be used to describe linear relationships. 5.2 Pearson Correlation Coefficient The Pearson correlation coefficient is a statistical measure that quantifies the direction and strength of a linear relationship between two variables. The sign of the Pearson correlation coefficient indicates the direction and the absolute value, ranging from \\(0\\) to \\(1\\), indicates the strength of the relationship. 5.2.1 Calculating Coefficient The Pearson correlation coefficient is calculated as follows: \\[ \\rho = \\frac{Cov(X, Y)}{\\sigma_{X} \\sigma_{Y}} = \\frac{\\sigma_{XY}}{\\sigma_{X} \\sigma_{Y}} \\] Note that \\(\\rho\\) denotes the population Pearson correlation coefficient, while \\(r\\) denotes the sample correlation coefficient. The latter is calculated as follows: \\[ r = \\frac{s_{XY}}{s_{X} s_{Y}} \\] Dividing the covariance between \\(X\\) and \\(Y\\) by the product of the standard deviations of \\(X\\) and \\(Y\\) ensures that the correlation coefficient is not dependent on the scale of the variables \\(X\\) and \\(Y\\) and that \\(\\rho \\in [-1, +1]\\). If \\(\\rho = +1\\), then all of the data falls on a single straight line with a positive slope. Some examples: # Create a sample dataset dat &lt;- tibble(X = runif(n = 20, min = 10, max = 80)) dat &lt;- add_column(dat, Y1 = 0.30*dat$X) dat &lt;- add_column(dat, Y2 = 3.00*dat$X) dat &lt;- add_column(dat, Y3 = 30.0*dat$X) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y1), formula = &quot;y ~ x&quot;, color = &quot;darkred&quot;, se = F) + geom_point(aes(y = Y2), color = &quot;darkgreen&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y2), formula = &quot;y ~ x&quot;, color = &quot;darkgreen&quot;, se = F) + geom_point(aes(y = Y3), color = &quot;darkblue&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y3), formula = &quot;y ~ x&quot;, color = &quot;darkblue&quot;, se = F) + labs(x = &quot;X&quot;, y = &quot;Y&quot;) + theme_classic() Note that these examples illustrate a functional relationship. The relationship between \\(X\\) and each of the \\(Y\\) variables (\\(Y1\\), \\(Y2\\), \\(Y3\\)) can be described by an equation and all the observations fall on the line of the relationship. If we compute the Pearson correlation coefficients for each of these three pairs of variables, we see that it is each time equal to \\(r = +1\\). cor(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## [1] 1 cor(x = dat$X, y = dat$Y2, method = &quot;pearson&quot;) ## [1] 1 cor(x = dat$X, y = dat$Y3, method = &quot;pearson&quot;) ## [1] 1 If we now use a negative slope, we will see that the Pearson correlation coefficient for each of these functional relationships equals \\(r = -1\\). # Create a sample dataset dat &lt;- tibble(X = runif(n = 20, min = 10, max = 80)) dat &lt;- add_column(dat, Y1 = -0.30*dat$X) dat &lt;- add_column(dat, Y2 = -3.00*dat$X) dat &lt;- add_column(dat, Y3 = -30.0*dat$X) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y1), formula = &quot;y ~ x&quot;, color = &quot;darkred&quot;, se = F) + geom_point(aes(y = Y2), color = &quot;darkgreen&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y2), formula = &quot;y ~ x&quot;, color = &quot;darkgreen&quot;, se = F) + geom_point(aes(y = Y3), color = &quot;darkblue&quot;) + geom_smooth(method = &quot;lm&quot;, aes(y = Y3), formula = &quot;y ~ x&quot;, color = &quot;darkblue&quot;, se = F) + labs(x = &quot;X&quot;, y = &quot;Y&quot;) + theme_classic() cor(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## [1] -1 cor(x = dat$X, y = dat$Y2, method = &quot;pearson&quot;) ## [1] -1 cor(x = dat$X, y = dat$Y3, method = &quot;pearson&quot;) ## [1] -1 We see that the absolute value of the Pearson correlation coefficient equals \\(1\\) for the six examples. This indicates that these are very strong linear relationships. But what does it mean for a relationship between two variables to be strong? The strength of a relationship between two variables is an indication of how much information a given value for one variables gives us about the value of the other variable. If we see that for a given value of \\(X\\) the range of values for \\(Y\\) is relatively large, and this for every value of \\(X\\), then we cannot say that \\(X\\) carries much information about \\(Y\\). If, however, the range of values for \\(Y\\) is relatively small for every value of \\(X\\), then we can state that \\(X\\) carries a lot of information about the value of \\(Y\\) and the relationship is quite strong. We will showcase with a few examples. # Create a sample dataset dat &lt;- tibble(X = runif(100, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 + 5*dat$X + rnorm(100, sd = 1)) dat &lt;- add_column(dat, Y2 = -5 + 5*dat$X + rnorm(100, sd = 10)) dat &lt;- add_column(dat, Y3 = -5 + 5*dat$X + rnorm(100, sd = 25)) For the following three positive linear relationships, we see that the amount of scatter around the regression line is different for the three examples. The amount of scatter around the regression line is an indication of how much information the variable \\(X\\) carries about the variable \\(Y\\). If we look at any given value for \\(X\\), we see that the range of observed values for \\(Y1\\) is relatively small. For \\(Y2\\), the range of observed \\(Y2\\) values is larger. And for \\(Y3\\), the range of observed \\(Y3\\) values is relatively large. Therefore, we can state that the relationship between \\(X\\) and \\(Y1\\) is strong, between \\(X\\) and \\(Y2\\) is moderate, and between \\(X\\) and \\(Y3\\) is weak. # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = 5, color = &quot;darkred&quot;) + labs(x = &quot;X&quot;, y = &quot;Y1&quot;) + scale_y_continuous(limits = c(60, 250)) + theme_classic() # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y2), color = &quot;darkgreen&quot;) + geom_abline(intercept = -5, slope = 5, color = &quot;darkgreen&quot;) + labs(x = &quot;X&quot;, y = &quot;Y2&quot;) + scale_y_continuous(limits = c(60, 250)) + theme_classic() # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y3), color = &quot;darkblue&quot;) + geom_abline(intercept = -5, slope = 5, color = &quot;darkblue&quot;) + labs(x = &quot;X&quot;, y = &quot;Y3&quot;) + scale_y_continuous(limits = c(60, 250)) + theme_classic() The absolute values of the Pearson correlation coefficient are indicative of the strength of the linear relationship. Therefore, we expect the largest Pearson correlation coefficient for \\(X\\) and \\(Y1\\), a smaller one for \\(X\\) and \\(Y2\\), and the smallest one for \\(X\\) and \\(Y3\\). cor(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## [1] 0.9993038 cor(x = dat$X, y = dat$Y2, method = &quot;pearson&quot;) ## [1] 0.9429303 cor(x = dat$X, y = dat$Y3, method = &quot;pearson&quot;) ## [1] 0.703926 5.2.2 Hypothesis Testing All of the above examples on the Pearson correlation coefficient made use of samples of data to calculate the correlation. When analyzing data, we must be be aware that often we are analyzing a sample of data which is drawn from a larger population. For this population, there is a true Pearson correlation coefficient \\(\\rho\\), which is an unknown constant. To estimate \\(\\rho\\), we draw a random sample from the population and use an estimator to produce an estimate \\(\\hat{\\rho}\\). We have already seen the estimator in this chapter. It is: \\[ r = \\frac{s_{XY}}{s_{X} s_{Y}} \\] We must remember that the Pearson correlation coefficient \\(r\\) is an estimator for the true Pearson correlation coefficient \\(\\rho\\) in the population. The latter is an unknown constant. We want to estimate this unknown constant and therefore we sample data from the population and analyze this data to produce an estimate of the true value of \\(\\rho\\). Using this, we can perform a \\(t\\)-test to test for the existence of a linear relationship between two variables. The null hypothesis states that the population correlation coefficient \\(\\rho = 0\\) and that there is no linear relationship: \\[ H_0 : \\rho = 0 \\] The alternative hypothesis states: \\[ H_a : \\rho \\neq 0 \\] The test statistic \\(T\\) for this test is: \\[ T = \\frac{r \\cdot \\sqrt{n - 2}}{\\sqrt{1 - R^2}} \\] The test statistic \\(T\\) follows a \\(t\\)-distribution with \\(n - 2\\) degrees of freedom. cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = 265.15, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9989636 0.9995323 ## sample estimates: ## cor ## 0.9993038 cor.test(x = dat$X, y = dat$Y2, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y2 ## t = 28.032, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9162004 0.9613063 ## sample estimates: ## cor ## 0.9429303 cor.test(x = dat$X, y = dat$Y3, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y3 ## t = 9.811, df = 98, p-value = 3.112e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.5889364 0.7909801 ## sample estimates: ## cor ## 0.703926 In these three examples, we see that the \\(p\\)-values are all statistically significant at \\(\\alpha = 0.05\\). If we look at the test statistic \\(T\\), we see that both sample size \\(n\\) and the estimate \\(r\\) affect the test statistic. We see that larger sample sizes and larger estimates of \\(\\rho\\) result in a larger test statistic. In the following example, we keep the random part of the relationship relatively small. So we are drawing from a population with a strong negative linear relationship. We will use varying sample sizes (from small to large). # Create a sample dataset n &lt;- 3 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 1)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -120.08, df = 1, p-value = 0.005301 ## alternative hypothesis: true correlation is not equal to 0 ## sample estimates: ## cor ## -0.9999653 # Create a sample dataset n &lt;- 30 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 1)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -136.97, df = 28, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9996494 -0.9984158 ## sample estimates: ## cor ## -0.9992546 # Create a sample dataset n &lt;- 50 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 1)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -236.6, df = 48, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9997581 -0.9992411 ## sample estimates: ## cor ## -0.9995715 We will now increae the variability around the regression line of the linear relationship. # Create a sample dataset n &lt;- 3 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 30)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -3.9069, df = 1, p-value = 0.1595 ## alternative hypothesis: true correlation is not equal to 0 ## sample estimates: ## cor ## -0.9687692 # Create a sample dataset n &lt;- 30 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 30)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -4.5246, df = 28, p-value = 0.0001016 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.8185127 -0.3781540 ## sample estimates: ## cor ## -0.6498847 # Create a sample dataset n &lt;- 50 dat &lt;- tibble(X = runif(n, min = 20, max = 40)) dat &lt;- add_column(dat, Y1 = -5 - 5*dat$X + rnorm(n, sd = 30)) # Visualize the relationships ggplot(dat, aes(x = X)) + geom_point(aes(y = Y1), color = &quot;darkred&quot;) + geom_abline(intercept = -5, slope = -5, color = &quot;darkred&quot;) + theme_classic() cor.test(x = dat$X, y = dat$Y1, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y1 ## t = -7.2289, df = 48, p-value = 3.281e-09 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.8329269 -0.5551802 ## sample estimates: ## cor ## -0.721961 5.2.3 No Relationship Assume there is no relationship between \\(X\\) and \\(Y\\), then the true Pearson correlation coefficient \\(\\rho = 0\\). Therefore, the null hypothesis should not be rejected. # Create a sample dataset dat &lt;- tibble(X = runif(100, min = 20, max = 40), Y = rnorm(100, mean = -5, sd = 1)) # Visualize the relationships ggplot(dat, aes(x = X, y = Y)) + geom_point(color = &quot;darkred&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;, se = F) + theme_classic() cor.test(x = dat$X, y = dat$Y, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y ## t = -0.51726, df = 98, p-value = 0.6061 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.2460756 0.1457322 ## sample estimates: ## cor ## -0.05217951 Another example. # Create a sample dataset dat &lt;- tibble(X = rnorm(100), Y = rnorm(100)) # Visualize the relationships ggplot(dat, aes(x = X, y = Y)) + geom_point(color = &quot;darkred&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;, se = F) + theme_classic() cor.test(x = dat$X, y = dat$Y, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: dat$X and dat$Y ## t = 0.94208, df = 98, p-value = 0.3485 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.1036095 0.2858359 ## sample estimates: ## cor ## 0.09473661 5.2.4 Orange Dataset If we go back to our case study from the trees dataset. ggplot(Orange, aes(x = age, y = circumference)) + geom_point() + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;, se = FALSE) + labs(x = &quot;Age (days)&quot;, y = &quot;Circumference (mm)&quot;) + theme_classic() We already described this relationship. We can now calculate the Pearson correlation coefficient manually. First, we must calculate the covariance. Let the random variable \\(X\\) denote the age and \\(Y\\) the circumference. # Calculate the quantities required to calculate the covariance orange_rel &lt;- as_tibble(Orange) %&gt;% select(X = age, Y = circumference) %&gt;% mutate(X_bar = mean(X), Y_bar = mean(Y)) %&gt;% mutate(X_minus_X_bar = X - X_bar, Y_minus_Y_bar = Y - Y_bar) %&gt;% mutate(cross_product = X_minus_X_bar * Y_minus_Y_bar) kable(orange_rel) X Y X_bar Y_bar X_minus_X_bar Y_minus_Y_bar cross_product 118 30 922.1429 115.8571 -804.14286 -85.8571429 69041.40816 484 58 922.1429 115.8571 -438.14286 -57.8571429 25349.69388 664 87 922.1429 115.8571 -258.14286 -28.8571429 7449.26531 1004 115 922.1429 115.8571 81.85714 -0.8571429 -70.16327 1231 120 922.1429 115.8571 308.85714 4.1428571 1279.55102 1372 142 922.1429 115.8571 449.85714 26.1428571 11760.55102 1582 145 922.1429 115.8571 659.85714 29.1428571 19230.12245 118 33 922.1429 115.8571 -804.14286 -82.8571429 66628.97959 484 69 922.1429 115.8571 -438.14286 -46.8571429 20530.12245 664 111 922.1429 115.8571 -258.14286 -4.8571429 1253.83673 1004 156 922.1429 115.8571 81.85714 40.1428571 3285.97959 1231 172 922.1429 115.8571 308.85714 56.1428571 17340.12245 1372 203 922.1429 115.8571 449.85714 87.1428571 39201.83673 1582 203 922.1429 115.8571 659.85714 87.1428571 57501.83673 118 30 922.1429 115.8571 -804.14286 -85.8571429 69041.40816 484 51 922.1429 115.8571 -438.14286 -64.8571429 28416.69388 664 75 922.1429 115.8571 -258.14286 -40.8571429 10546.97959 1004 108 922.1429 115.8571 81.85714 -7.8571429 -643.16327 1231 115 922.1429 115.8571 308.85714 -0.8571429 -264.73469 1372 139 922.1429 115.8571 449.85714 23.1428571 10410.97959 1582 140 922.1429 115.8571 659.85714 24.1428571 15930.83673 118 32 922.1429 115.8571 -804.14286 -83.8571429 67433.12245 484 62 922.1429 115.8571 -438.14286 -53.8571429 23597.12245 664 112 922.1429 115.8571 -258.14286 -3.8571429 995.69388 1004 167 922.1429 115.8571 81.85714 51.1428571 4186.40816 1231 179 922.1429 115.8571 308.85714 63.1428571 19502.12245 1372 209 922.1429 115.8571 449.85714 93.1428571 41900.97959 1582 214 922.1429 115.8571 659.85714 98.1428571 64760.26531 118 30 922.1429 115.8571 -804.14286 -85.8571429 69041.40816 484 49 922.1429 115.8571 -438.14286 -66.8571429 29292.97959 664 81 922.1429 115.8571 -258.14286 -34.8571429 8998.12245 1004 125 922.1429 115.8571 81.85714 9.1428571 748.40816 1231 142 922.1429 115.8571 308.85714 26.1428571 8074.40816 1372 174 922.1429 115.8571 449.85714 58.1428571 26155.97959 1582 177 922.1429 115.8571 659.85714 61.1428571 40345.55102 We can now calculate the covariance as follows: \\[ Cov(X, Y) = \\frac{\\sum_{i = 1}^{n} (X_i - \\overline{X})(Y_i - \\overline{Y})}{n - 1} \\] Which gives us: sum(orange_rel$cross_product) / (nrow(orange_rel) - 1) ## [1] 25831.02 or: cov(orange_rel$X, orange_rel$Y) ## [1] 25831.02 We can now divide the covariance between \\(X\\) and \\(Y\\) by the product of the sample standard deviations of \\(X\\) and \\(Y\\): (sum(orange_rel$cross_product) / (nrow(orange_rel) - 1)) / (sd(orange_rel$X) * sd(orange_rel$Y)) ## [1] 0.9135189 or: cor(x = orange_rel$X, y = orange_rel$Y, method = &quot;pearson&quot;) ## [1] 0.9135189 Before interpreting the estimate of the Pearson correlation coefficient, we can perform a hypothesis test to check if it differs from 0 at a significance level of \\(\\alpha = 0.05\\). The null and alternative hypotheses state: \\[ H_0 : \\rho = 0 \\text{ vs. } H_a : \\rho \\neq 0 \\] We perform the hypothesis test: cor.test(x = Orange$age, y = Orange$circumference, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: Orange$age and Orange$circumference ## t = 12.9, df = 33, p-value = 1.931e-14 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.8342364 0.9557955 ## sample estimates: ## cor ## 0.9135189 To conclude, we can state that the visualization of the observations and a LOESS curve indicate a positive linear relationship between age and circumference of trees. The Pearson correlation coefficient estimate of \\(r = 0.9135189\\) (\\(p &lt; 0.0001\\)) indicates a very strong positive linear relation between age and circumference. 5.3 Spearman Rank Correlation The Spearman rank correlation coefficient can be used for ordinal or rank-ordered data. To calculate the Spearman correlation for variables \\(X\\) and \\(Y\\), we first must obtain the rank for every value per variable. For this, we order the values from large to small and assign a rank to each value. Let the random variable \\(X\\) denote the age in days and \\(Y\\) denote the circumference in millimeters. Let \\(R_{xi}\\) denote the rank for the observed value \\(x_i\\) and \\(R_{yi}\\) the rank for the observed value \\(y_i\\). orange_spm &lt;- as_tibble(Orange) %&gt;% select(X = age, Y = circumference) %&gt;% mutate(X_rank = rank(X), Y_rank = rank(Y)) kable(orange_spm) X Y X_rank Y_rank 118 30 3 2.0 484 58 8 8.0 664 87 13 13.0 1004 115 18 17.5 1231 120 23 19.0 1372 142 28 23.5 1582 145 33 25.0 118 33 3 5.0 484 69 8 10.0 664 111 13 15.0 1004 156 18 26.0 1231 172 23 28.0 1372 203 28 32.5 1582 203 33 32.5 118 30 3 2.0 484 51 8 7.0 664 75 13 11.0 1004 108 18 14.0 1231 115 23 17.5 1372 139 28 21.0 1582 140 33 22.0 118 32 3 4.0 484 62 8 9.0 664 112 13 16.0 1004 167 18 27.0 1231 179 23 31.0 1372 209 28 34.0 1582 214 33 35.0 118 30 3 2.0 484 49 8 6.0 664 81 13 12.0 1004 125 18 20.0 1231 142 23 23.5 1372 174 28 29.0 1582 177 33 30.0 With the ranks, we can then calculate the difference in ranks for every observation as follows: \\[ d_i = R_{xi} - R_{yi} \\] Subsequently, we can calculate the sample Spearman correlation: \\[ r_s = 1 - \\frac{6 \\sum_{i = 1}^{n} d_{i}^{2}}{n(n^2 - 1)} \\] We can perform a hypothesis test with the following null and alternative hypotheses: \\[ H_0 : \\rho = 0 \\text{ vs. } H_a : \\rho \\neq 0 \\] Under the null hypothesis and if the sample size is large enough (\\(n \\geq 10\\)), the Spearman rank correlation coefficient is approximately normally distributed with \\(\\mu_{r_s} = 0\\) and \\(\\sigma_{r_s} = \\sqrt{\\frac{1}{n - 1}}\\). Using this, we can calculate the test statistic \\(T\\): \\[ T = \\frac{r_s - \\mu_{r_s}}{\\sigma_{r_s}} \\] We can continue with the Orange dataset: orange_spm &lt;- orange_spm %&gt;% mutate(delta = X_rank - Y_rank, delta_2 = delta^2) kable(orange_spm) X Y X_rank Y_rank delta delta_2 118 30 3 2.0 1.0 1.00 484 58 8 8.0 0.0 0.00 664 87 13 13.0 0.0 0.00 1004 115 18 17.5 0.5 0.25 1231 120 23 19.0 4.0 16.00 1372 142 28 23.5 4.5 20.25 1582 145 33 25.0 8.0 64.00 118 33 3 5.0 -2.0 4.00 484 69 8 10.0 -2.0 4.00 664 111 13 15.0 -2.0 4.00 1004 156 18 26.0 -8.0 64.00 1231 172 23 28.0 -5.0 25.00 1372 203 28 32.5 -4.5 20.25 1582 203 33 32.5 0.5 0.25 118 30 3 2.0 1.0 1.00 484 51 8 7.0 1.0 1.00 664 75 13 11.0 2.0 4.00 1004 108 18 14.0 4.0 16.00 1231 115 23 17.5 5.5 30.25 1372 139 28 21.0 7.0 49.00 1582 140 33 22.0 11.0 121.00 118 32 3 4.0 -1.0 1.00 484 62 8 9.0 -1.0 1.00 664 112 13 16.0 -3.0 9.00 1004 167 18 27.0 -9.0 81.00 1231 179 23 31.0 -8.0 64.00 1372 209 28 34.0 -6.0 36.00 1582 214 33 35.0 -2.0 4.00 118 30 3 2.0 1.0 1.00 484 49 8 6.0 2.0 4.00 664 81 13 12.0 1.0 1.00 1004 125 18 20.0 -2.0 4.00 1231 142 23 23.5 -0.5 0.25 1372 174 28 29.0 -1.0 1.00 1582 177 33 30.0 3.0 9.00 We now calculate the sample Spearman rank correlation coefficient: n &lt;- nrow(orange_spm) 1 - ((6 * sum(orange_spm$delta_2)) / (n * (n^2 - 1))) ## [1] 0.9073529 or: cor(x = orange_spm$X, y = orange_spm$Y, method = &quot;spearman&quot;) ## [1] 0.9064294 We can now continue and calculate the test statistic \\(T\\). First, we must calculate the parameters of the null distribution: \\[ \\mu_{r_s} = 0 \\] and: \\[ \\sigma_{r_s} = \\sqrt{\\frac{1}{n - 1}} = \\sqrt{\\frac{1}{35 - 1}} = 0.1714986 \\] Our test statistic \\(T\\) is: \\[ T = \\frac{r_s - \\mu_{r_s}}{\\sigma_{r_s}} = \\frac{0.9073529 - 0}{0.1714986} = 5.290731 \\] We calculate the \\(p\\)-value as follows: # Calculate the statistics for the null distribution T_mu &lt;- 0 T_se &lt;- sqrt(1 / (35 - 1)) T_stat &lt;- (0.9073529 - T_mu) / T_se p_val &lt;- pnorm(q = T_stat, lower.tail = FALSE)*2 p_val ## [1] 1.218284e-07 Or we can also calculate as follows: # Calculate the test statistic cor.test(x = orange_spm$X, y = orange_spm$Y, method = &quot;spearman&quot;, exact = F) ## ## Spearman&#39;s rank correlation rho ## ## data: orange_spm$X and orange_spm$Y ## S = 668.09, p-value = 6.712e-14 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.9064294 Using a sample dataset: # Create a sample dataset dat &lt;- tibble(X = rnorm(15), Y = rnorm(15)) dat ## # A tibble: 15 × 2 ## X Y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.396 -0.759 ## 2 0.379 0.0820 ## 3 -1.18 -0.347 ## 4 0.652 0.243 ## 5 1.47 1.68 ## 6 -1.06 0.724 ## 7 0.915 -0.711 ## 8 0.900 0.486 ## 9 0.564 0.822 ## 10 -0.376 0.492 ## 11 -1.00 0.308 ## 12 0.423 -0.867 ## 13 -0.0149 -0.0605 ## 14 -2.58 1.44 ## 15 0.318 1.46 dat &lt;- dat %&gt;% mutate(X_rank = rank(X), Y_rank = rank(Y)) %&gt;% mutate(delta = X_rank - Y_rank, delta_2 = delta^2) n &lt;- nrow(dat) 1 - ((6 * sum(dat$delta_2)) / (n * (n^2 - 1))) ## [1] -0.07142857 or: cor(x = dat$X, y = dat$Y, method = &quot;spearman&quot;) ## [1] -0.07142857 # Calculate the statistics for the null distribution T_mu &lt;- 0 T_se &lt;- sqrt(1 / (nrow(dat) - 1)) T_stat &lt;- (0.15 - T_mu) / T_se p_val &lt;- pnorm(q = T_stat, lower.tail = FALSE)*2 p_val ## [1] 0.5746281 # Calculate the test statistic cor.test(x = dat$X, y = dat$Y, method = &quot;spearman&quot;, exact = F) ## ## Spearman&#39;s rank correlation rho ## ## data: dat$X and dat$Y ## S = 600, p-value = 0.8003 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## -0.07142857 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
