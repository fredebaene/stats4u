# Logistic Models and Deviance

```{r echo = FALSE}
library(tidyverse)
```

When fitting a logistic regression model to a data set, R outputs several key
metrics, including the **null deviance** and the **residual deviance**, along
with their respective degrees of freedom. This chapter aims to clarify the
calculation methods and meaning of these deviance values. We will use the first
20 observations from a heart disease data set, fit a logistic regression model
to this data, and manually compute the deviance values to illustrate how they
are actually calculated.

```{r}
# Read the data and only retain the first 20 observations
heart <- readr::read_csv(file = "data/heart.xls", show_col_types = FALSE)
heart <- head(heart, 20)
```

## Models

To understand what these deviance values are, we must first understand the
following three models:

* the **null** model ($M_0$),
* the **proposed** model ($M_p$),
* and the **saturated** model ($M_s$).

These three models are used to calculate the two deviance values outputted by R,
i.e., the null deviance (the deviance of $M_0$) and the residual deviance(the
deviance of $M_p$).

### Proposed Model

The proposed model $M_p$ is the model of interest. In our example, it is used to
estimate the probability of heart disease for each patient, conditioning on that
patient's age. The model is represented as follows:

$$
logit(\pi_i) = \beta_0 + \beta_1 \times Age_i + \epsilon_i
$$

with $\pi_i = P(HD_i = 1 | Age_i)$ the conditional probability of a heart
disease for patient $i$, $Age_i$ the age of patient $i$, and $\epsilon_i$ the
random error of the model for the $i$'th observation. We can fit the proposed
model as follows:

```{r}
fit_p <- glm(
  formula = HeartDisease ~ Age,
  data = heart,
  family = binomial(link = "logit")
)
```

```{r echo = FALSE}
summary(fit_p)
```

We obtain the null deviance $D_0 = `r round(fit_p$null.deviance, 1)`$ and the
residual deviance $D_p = `r round(fit_p$deviance, 1)`$.

### Null Model

The null model $M_0$ is a model without any predictors. It only contains an 
intercept $\beta_0$:

$$
logit(\pi_i) = \beta_0 + \epsilon_i
$$

The estimated probability of heart disease is the same for each patient. The 
null model $M_0$ is fitted as follows:

```{r}
fit_0 <- glm(
  formula = HeartDisease ~ 1,
  data = heart,
  family = binomial(link = "logit")
)
```

```{r echo = FALSE}
summary(fit_0)
```

R outputs the null deviance $D_0$ and the residual deviance $D_p$, which both 
equal `r round(fit_0$null.deviance, 1)`. They are the same because $M_p$ is the 
same as $M_0$ (neither of them contains a predictor, only an intercept).

### Saturated Model

The saturated model $M_s$ is a model that perfectly fits data. This model
perfectly predicts the outcome for each patient in the data set. If patient $i$
has a heart disease ($HD_i = 1$), then $\hat{\pi_i} = 1$. If patient $i$ does
not have a heart disease ($HD_i = 0$), then $\hat{\pi_i} = 0$. We will not fit a
saturated model $M_s$ as such to the data, but continue our explanation knowing
the fact that the model perfectly predicts the outcome for each observation $i$.

### Models and Deviance

Since the saturated model $M_s$ perfectly fits the data, it serves as our
reference point for assessing goodness-of-fit. The proposed model $M_p$ and the
null model $M_0$ are compared to the saturated model in terms of their
goodness-of-fit. The result of these comparisons is a deviance value for each
model. The deviance quantifies the degree to which the null model and the
proposed model deviate from the saturated model in terms of goodness-of-fit. To
calculate the actual deviance, we utilize the likelihoods and log-likelihoods of
the models (see below for further details).

## Likelihood and Log-Likelihood

The likelihood is a probability that indicates how likely it is that the
parameters take on particular values **given a particular sample**. As the
sample is given, the values for the outcome and predictor are not variables. The
parameter values are the variables. We can represent the likelihood function as
follows:

$$
L(\theta | (x_1, y_1), (x_2, y_2), ..., (x_n, y_n)) 
= \prod_{i = 1}^{n} f_Y(y_i; x_i, \theta)
$$

with $\theta$ the parameters of interest, $n$ the sample size, and 
$f_Y(y; x, \theta)$ the probability mass function for the response. Calculating 
the likelihood involves taking the product of many probabilities (i.e., values 
between 0 and 1), and therefore results in a very small value. For computational 
reasons, often the log-likelihood is calculated instead of the likelihood.

$$
\begin{aligned}
l(\theta | (x_1, y_1), (x_2, y_2), ..., (x_n, y_n)) 
&= log[\prod_{i = 1}^{n} f_Y(y_i; x_i, \theta)] \\
&= \sum_{i = 1}^{n} log(f_Y(y_i; x_i, \theta))
\end{aligned}
$$

### Proposed Model

We start by calculating the likelihood and log-likelihood of $M_p$. 

$$
L(\theta | (x_1, y_1), (x_2, y_2), ..., (x_n, y_n)) = \prod_{i = 1}^{n} f_Y(y_i; x_i, \theta)
$$

with $\theta$ representing the parameters that are the variables and $n$ the
sample size. To define the likelihood function, we must first determine the
probability mass function for the response. The PMF
defines the probability distribution of a discrete random variable. Remember
that a logistic regression model is used to predict the outcome for a discrete
random variable, and more specifically a binary random variable that follows a
Bernoulli distribution:

$$
HD \sim Bernoulli(\pi)
$$

The probability mass function (PMF) of a Bernoulli distribution is the
following:

$$
f_Y(y; \pi) = \pi^{y} \times (1 - \pi)^{1 - y}
$$

Therefore, we use the PMF of the Bernoulli distribution for the likelihood 
function:

$$
L(\theta | (x_1, y_1), (x_2, y_2), ..., (x_n, y_n)) 
= \prod_{i = 1}^{n} \pi_{i}^{y_i} \times (1 - \pi_i)^{1 - y_i}
$$

with $\theta$ representing the regression coefficients of the logistic
regression model. We can now also define the log-likelihood function:

$$
\begin{aligned}
l(\theta | (x_1, y_1), (x_2, y_2), ..., (x_n, y_n)) 
&= log[\prod_{i = 1}^{n} \pi_{i}^{y_i} \times (1 - \pi_i)^{1 - y_i}] \\
&= \sum_{i = 1}^{n} [y_i \times log(\pi_i) + (1 - y_i) \times log(1 - \pi_i)]
\end{aligned}
$$

Using our fitted model $M_p$, how can we now obtain the log-likelihood? We start
by obtaining the estimated probability of observing the actual outcome for each
observation in the data set. We focus on the third observation in the data set.
We see that $Age_3 = 37$ and $HD_3 = 0$. We can use our model to obtain the 
probability of a heart disease:

```{r}
# Estimate the probability of a heart disease for i = 3
hd_prob_003 <- predict(fit_p, type = "response")[3]
hd_prob_003
```

We see that this probability is 
$P(HD_3 = 1 | Age_i = 37) = \pi_i = `r round(hd_prob_003, 2)`$. Assume that 
our sample only comprises this one patient, then the likelihood would be:

$$
\begin{aligned}
L(\theta | (Age_i, HD_i)) 
&= \prod_{i = 1}^{n} f_Y(y_i; x_i, \theta) \\
&= \prod_{i = 1}^{n} \pi_{i}^{y_i} \times (1 - \pi_i)^{1 - y_i} \\
&= 0.3105577^{0} \times (1 - 0.3105577)^{1 - 0} \\
&= 1 \times 0.6894423^{1} \\
&= 0.6894423
\end{aligned}
$$

So, to calculate the likelihood, first, we calculate the estimated probability 
with which the actual outcome occurs.

```{r}
# Create a copy of the data frame as to not overwrite the original data
heart_p <- heart

# Calculate the probability of a patient having or not having a heart disease 
# using the proposed model (i.e., given the patient's age)
heart_p$EstProb_HD_1 <- predict(fit_p, type = "response")
heart_p$EstProb_HD_0 <- 1 - predict(fit_p, type = "response")

heart_p <- heart_p %>%
  dplyr::mutate(
    ProbOutcome = EstProb_HD_1^HeartDisease * EstProb_HD_0^(1 - HeartDisease),
    LogProbOutcome = log(ProbOutcome)
  )
```

```{r echo = FALSE}
cols <- c(
  "Age",
  "HeartDisease",
  "EstProb_HD_1",
  "EstProb_HD_0",
  "ProbOutcome",
  "LogProbOutcome"
)
col_names <- c(
  "Age",
  "Heart Disease",
  "P(HD = 1)",
  "P(HD = 0)",
  "P(Outcome)",
  "Log(P(Outcome))"
)

knitr::kable(x = heart_p[cols], col.names = col_names)
```

The above data show, given the fitted model, the probability with which each
actual outcome occurs, given the patient's age, and also the log-transformed
probability. To obtain the log-likelihood of the fitted model $LL_p$, we can now
just sum these log-transformed probabilities:

```{r}
# Obtain the log-likelihood for the proposed model
ll_p <- sum(heart_p$LogProbOutcome)
ll_p
```

### Null Model

We can also do this for the null model. Remember that the fitted probability 
of a heart disease will be the same for each patient, as we do not take into 
account any patient information. We can then compute the log-likelihood of the 
null model as follows:

```{r}
# Create a copy of the data frame as to not overwrite the original data
heart_0 <- heart

# Calculate the probability of a patient having or not having a heart disease 
# using the null model (i.e., not taking into account any patient information)
heart_0$EstProb_HD_1 <- predict(fit_0, type = "response")
heart_0$EstProb_HD_0 <- 1 - predict(fit_0, type = "response")

heart_0 <- heart_0 %>%
  dplyr::mutate(
    ProbOutcome = EstProb_HD_1^HeartDisease * EstProb_HD_0^(1 - HeartDisease),
    LogProbOutcome = log(ProbOutcome)
  )
```

```{r echo = FALSE}
knitr::kable(x = heart_0[cols], col.names = col_names)
```

We obtain the log-likelihood of the null model $LL_0$ as follows:

```{r}
ll_0 <- sum(heart_0$LogProbOutcome)
ll_0
```

### Saturated Model

We can follow the same procedure for $M_s$. The saturated model perfectly fits 
the data, so the estimated probability of heart disease reflects the actual 
outcome. In other words, if $HD_i = 1$, then $P(HD_i = 1) = 1$. And if 
$HD_i = 0$, then $P(HD_i = 1) = 0$.

```{r}
# Create a copy of the data frame as to not overwrite the original data
heart_s <- heart

# Calculate the probability of a patient having or not having a heart disease 
# using the saturated model (i.e., the model perfectly fits the data)
heart_s$EstProb_HD_1 <- heart_s$HeartDisease
heart_s$EstProb_HD_0 <- 1 - heart_s$HeartDisease

heart_s <- heart_s %>%
  dplyr::mutate(
    ProbOutcome = EstProb_HD_1^HeartDisease * EstProb_HD_0^(1 - HeartDisease),
    LogProbOutcome = log(ProbOutcome)
  )
```

```{r echo = FALSE}
knitr::kable(x = heart_s[cols], col.names = col_names)
```

Finally, we obtain the log-likelihood for the saturated model $LL_s$:

```{r}
ll_s <- sum(heart_s$LogProbOutcome)
ll_s
```

## Deviance

As stated earlier, the saturated model $M_s$ perfectly fits the data, and 
therefore serves as our reference point for goodness-of-fit. We now use the 
log-likelihood values of these models to calculate the deviance. For the 
residual deviance $D_p$, we have:

$$
\begin{aligned}
D_p &= 2 \times (LL_s - LL_p) \\
&= 2 \times (0 - (-13.18263)) \\
&= 2 \times 13.18263 \\
&= 26.36526
\end{aligned}
$$

and for the null deviance $D_0$, we have:

$$
\begin{aligned}
D_0 &= 2 \times (LL_s - LL_0) \\
&= 2 \times (0 - (-13.46023)) \\
&= 2 \times 13.46023 \\
&= 26.92047
\end{aligned}
$$

As we can see, the deviance is two times the difference between the
log-likelihood of the saturated model and the log-likelihood of the proposed
model for the residual deviance or the log-likelihood of the null model for the
null deviance. The deviance quantifies how much the null and proposed models
differ from the saturated model in terms of goodness-of-fit.

In a following chapter, we will explain how we can use these deviance value to 
perform statistical hypothesis tests to formally compare the proposed model to 
the null model.
