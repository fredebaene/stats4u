# Classification

```{r echo = FALSE}
library(tidyverse)
```

Classification problems occur in settings where the response $Y$ is qualitative
instead of quantitative. The set of possible values for the response $Y$ is
limited in size. Furthermore, if the response $Y$ can only take on one out of
two possible values, then the response $Y$ is binary.

## Introduction

For this chapter, we will use the **Heart Failure Prediction** data set.

```{r}
heart <- readr::read_csv(
  file = "data/heart.xls",
  show_col_types = FALSE
)
```

This data set comprises $n = `r nrow(heart)`$ observations. In this data set,
the response $HeartDisease$ can be represented by an indicator variable $HD$.
The variable $HD = 1$ if a patient has heart failure and $HD = 0$ if otherwise.

Note that the response variable does not take on a numerical value. There are
only two possible values: include **YES** ($HD = 1$) and **NO** ($HD = 0$).

## Bernoulli Random Variable

Because there are only two possible outcomes for the response $HD$, it is a a
Bernoulli random variable (binary random variable). The possible outcomes
include: success or failure (yes or no; 1 or 0). A Bernoulli random variable
follows a Bernoulli distribution (discrete probability distribution), which is
defined by a single parameter:

$$
HD \sim Bernoulli(\pi)
$$

with $\pi$ the probability of success in a single trial. We can use the data 
to estimate $\pi$, i.e., the probability that a patient has heart failure.

```{r}
# Estimate the probability of a heart disease
hd_prob <- mean(heart[["HeartDisease"]])
```

We obtain an estimate of $\hat{\pi} = `r round(hd_prob, 2)`$. In other words, 
$P(Y = 1) = `r round(hd_prob, 2)`$ is the probability of a patient in this 
sample having heart failure. We can visualize this as follows:

```{r echo = FALSE}
data = data.frame(x = c(0, 1), y = c(1 - hd_prob, hd_prob))

ggplot(data = data,aes(x = x, y = y)) +
  geom_col(width = .3) +
  theme_classic() +
  labs(title = "Probability Distribution for HD", x = "HD", y = "P(HD = Y)")
```

## Conditional Probabilities

We estimated that the probability of a single patient having a heart disease 
equals $\hat{\pi} = `r round(hd_prob, 2)`$. This probability does not take into 
account any additional patient information. We can, however, take into account 
this additional information on the patient to determine the probability of a 
heart disease. This probability is called a conditional probability.

For example, we can take into account the sex of a patient to determine the 
patient's probability of a heart disease:

$$
P(HD = 1 | SEX)
$$

Again, we can use the data to estimate these conditional probabilities:

```{r}
# Estimate conditional probabilities (conditioning on sex)
hd_prob_m <- mean(subset(x = heart$HeartDisease, subset = heart$Sex == "M"))
hd_prob_f <- mean(subset(x = heart$HeartDisease, subset = heart$Sex == "F"))
```

We see that the probability of heart disease for male patients is 
$P(HD = 1 | SEX = M) = `r round(hd_prob_m, 2)`$, while for female patients it 
is $P(HD = 1 | SEX = F) = `r round(hd_prob_f, 2)`$. It seems that male patients 
have a higher probability of having a heart disease compared to female patients.

Conditional probabilities allows us to take into account additional information 
to more accurately estimate the probability of a certain event (i.e., heart 
disease) occurring. The concept of conditional probabilities is important to 
understand how classifiers, such as logistic regression models, work.

## Logistic Regression Model

In this chapter, we will explore how conditional probabilities fit into a 
logistic regression model. Logistic regression models are nonlinear regression 
models and fit into the framework of generalized linear models. A logistic 
regression model can be used to estimate the probability of a certain event 
occurring taking into account additional information. In our case, a logistic 
regression model can be fitted to the hart disease data set to estimate the 
probability of a heart disease given the patient's sex.

First, we fit a logistic model. In this first example, the logistic model 
includes a single categorical predictor. Therefore, we must first ensure that 
the data set contains this predictor as a factor variable.

```{r}
# Create a new variable in the heart data set. This new variable is a factor 
# that represents the sex of a patient.
heart$FSex <- factor(heart$Sex)
```

We must also be aware of the coding scheme used for this categorical predictor. 
We can use a call to `contrasts()` to verify the coding scheme.

```{r}
contrasts(heart$FSex)
```

We see that a dummy/treatment coding scheme is used. The reference level is `F`, 
i.e., female patients form the reference group. We can now fit the logistic 
regression model.

```{r}
# Fit a logistic regression model with a single factor (sex)
fit_lrm <- glm(
  formula = HeartDisease ~ FSex,
  data = heart,
  family = binomial(link = "logit")
)
```

```{r echo = FALSE}
summary(fit_lrm)
```

The fitted model is the following:

$$
logit(\pi_i) = -1.0508 + 1.5904 \times X_{i\_male}
$$

with $\hat{\beta_0} = `r round(fit_lrm$coefficients[1], 2)`$, 
$\hat{\beta_1} = `r round(fit_lrm$coefficients[2], 2)`$, and $X_{i\_male}$ an 
indicator variable with $X_{i\_male} = 1$ if the patient is male and 0 otherwise.

In the following sections of this chapter, we will break down how to obtain the 
coefficient estimates and what they mean. We will also explain how we can use 
this fitted model to obtain the same conditional probabilities from the 
previous sections.

For an in-depth explanation what the values related to the deviance mean and 
how we can use these to assess how well the model fits the data, cfr. the 
appropriate section.

